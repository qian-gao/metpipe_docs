{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"metpipe documentation","text":"<p>metpipe provides a reproducible workflow for metabolomics and lipidomics data processing, from raw/feature-level data to quality control (QC), post-processing, evaluation, and statistical interpretation.</p>"},{"location":"#workflow","title":"Workflow","text":"<p>This documentation describes recommended computational practices for metabolomics/lipidomics, including:</p> <ul> <li>Study design and data organization (randomization, QC setup, data conversion, LC-MS data structure)</li> <li>Preprocessing (e.g., feature detection, filtering, missingness handling, normalization)</li> <li>Quality control (QC) and diagnostic reporting  </li> <li>Post-processing (e.g., feature cleaning, imputation, normalization)  </li> <li>Data evaluation (e.g., QC metrics, batch effects, drift) </li> </ul> <p>The goal is to support transparent and reproducible analyses across studies.</p>"},{"location":"#r-package-metpipe","title":"R package: metpipe","text":"<p>metpipe is an R package for metabolomics/lipidomics data processing. It implements core pipeline steps with an emphasis on:</p> <ul> <li>Consistent handling of sample metadata and analytical batches  </li> <li>Built-in QC and summary reporting  </li> <li>Standardized outputs that are easy to use in downstream statistical workflows  </li> </ul>"},{"location":"#web-application-data-explorer","title":"Web application: Data explorer","text":"<p>Data explorer is a Shiny web application built on top of metpipe. It provides an interactive interface to:</p> <ul> <li>Run common processing steps without writing code  </li> <li>Explore QC metrics and data structure  </li> <li>Generate plots and export processed datasets  </li> </ul> <p>Use the package for scripted, reproducible pipelines; use the app for interactive inspection of results.</p>"},{"location":"data_explorer/calibration/","title":"Calibration","text":""},{"location":"data_explorer/generate_sample_list/","title":"Randomization","text":""},{"location":"data_explorer/introduction/","title":"Introduction","text":""},{"location":"data_explorer/introduction/#introduction","title":"Introduction","text":"<p>Data explorer is a shiny web application based on metpipe package. It provides a series tools facilitating metabolomics data processing, exploration and visualization. Visit https://qiangao.shinyapps.io/metpipe_app/ for more information.</p>"},{"location":"data_explorer/match_library/","title":"Match library","text":""},{"location":"data_explorer/preprocessing/","title":"Preprocessing","text":""},{"location":"data_explorer/randomization/","title":"Randomization","text":""},{"location":"data_explorer/rt_mapping/","title":"Retention time mapping","text":""},{"location":"data_explorer/upload/","title":"Upload","text":""},{"location":"metpipe/introduction/","title":"Introduction","text":"<p>metpipe is an R package designed to provide a comprehensive and standardized pipeline for metabolomics data processing.</p> <p>This section introduces the use of metpipe for metabolomics and lipidomics data processing.</p>"},{"location":"metpipe/introduction/#system-requirements","title":"System Requirements","text":"<ul> <li>Docker version 4.47 or higher.</li> </ul> <p>Currently Docker has been setup in the following workstations:  </p> <ul> <li>Olaf</li> <li>Archibald </li> <li>Simba</li> <li>Boots</li> </ul> <p>Current version: 0.8</p>"},{"location":"metpipe/introduction/#special-notes-for-usage-in-ku-workstations","title":"Special notes for usage in KU workstations","text":"<ul> <li> <p>The procedure to start Docker</p> <ul> <li>Request Admin Privileges in Heimdal Agent</li> <li>Right click on Docker Desktop and choose Run as administrator</li> </ul> </li> <li> <p>Before start, check if Disk image location is setup in a drive other than C drive</p> <ul> <li>Open Docker, click Settings -&gt; Recsources -&gt; Advanced</li> <li>Check Disk image location, it should not be in C drive. It should be in e.g. E:\\DockerData\\DockerDesktopWSL</li> </ul> </li> <li> <p>After running, quit Docker before log out</p> </li> </ul>"},{"location":"metpipe/introduction/#database","title":"Database","text":"<p>Current libraries used in the workflow can be found in:   </p> <p>N:\\SUN-CBMR-Metabolomics\\Workflow\\metpipe_library. </p>"},{"location":"metpipe/pipeline/","title":"Overall procedures","text":""},{"location":"metpipe/pipeline/#standardized-workflow","title":"Standardized workflow","text":""},{"location":"metpipe/pipeline/#input","title":"Input","text":"<p>The only input needed is the subfolder name where the raw/mzML data is stored (e.g. PL_LIPS_PASEF). The subfolder naming should follow the standard naming convention as shown in the figure below (i.e. Matrix_Assay_method). The data file naming should follow the standard naming convention (i.e. Project ID_Assay Method Polarity_Date_Run Seq_Sample Name_Extraction Replicate_Technical Replicate, e.g. MP000_LIPSPASEFp_09092025_004_BL_1_1).</p>"},{"location":"metpipe/pipeline/#processing","title":"Processing","text":"<p>The workflow is executed in two sequential steps:</p> <ul> <li> <p>Prepare workflow   This step parses the input (the project subfolder containing raw/mzML files), inspects the files to infer assay- and file-specific settings, and generates all required parameters (for example: preprocessing options, library paths, and other assay-specific settings). All parameters are written to config.yml for use by the next step.</p> </li> <li> <p>Run workflow   This step reads the config.yml produced by prepare_workflow and executes the preprocessing and downstream processing (e.g., peak detection, alignment, and annotation) according to those parameters. Processing logs and result files are written to the project output directory.</p> </li> </ul>"},{"location":"metpipe/pipeline/#non-standardized-workflow","title":"Non-standardized workflow","text":"<p>For non-standardized workflows, the core processing steps are the same as for the standardized workflow. prepare_workflow still generates a default config.yml, but users may edit its parameters to accommodate non-standard inputs (for example, to point to an alternative library, change preprocessing or filtering thresholds, or modify assay-specific settings).</p>"},{"location":"metpipe/pipeline/#guidance-for-standardized-workflow","title":"Guidance for standardized workflow","text":""},{"location":"metpipe/pipeline/#input_1","title":"Input","text":"<p>The input is the subfolder name where the raw/mzML data is stored. The pipeline will automatically detect all the files in the subfolder and generate the parameters needed for processing. The in-house library and public spectral libraries will be automatically linked based on the assay and method. The pipeline will also detect the sample types (e.g., BL, PO, NIST) and running sequence based on the standardized sample naming convention. </p> <p>With the standardized naming convention, no further input is needed from the user.</p>"},{"location":"metpipe/pipeline/#prepare-workflow","title":"Prepare workflow","text":"<p>To prepare the workflow, run the following command in powershell:</p> <pre><code>docker run -it `\n    -v E:/:/mnt/e `\n    qiangao/metpipe:0.5 `\n    Rscript /wd/prepare_standard_workflow.R --raw \"/mnt/e/Projects/MP_workshop/raw/PL_LIPS_PASEF\"\n</code></pre> <p>This command mounts the E: drive to the Docker container and runs the <code>prepare_standard_workflow.R</code> script, specifying the raw data location. Adjust the raw folder paths and the dirve where the raw data is stored as necessary for your system. Note that the raw folder path inside the Docker container should start with <code>/mnt/</code> and use forward slashes.</p>"},{"location":"metpipe/pipeline/#check-generated-configyml","title":"Check generated config.yml","text":"<p>After running the prepare_workflow step, a <code>config.yml</code> file will be generated in the output directory. Review this file to ensure that all parameters are correctly set for your dataset. If you are using a non-standardized workflow, you may need to edit this file to adjust settings such as library paths or preprocessing options.</p> <p>The <code>config.yml</code> file contains all the necessary parameters for the subsequent processing step, including:</p>"},{"location":"metpipe/pipeline/#input-and-output-paths","title":"Input and output paths","text":"<ul> <li>path_raw: path to raw/mzML files</li> <li>path_result: path to output results </li> </ul>"},{"location":"metpipe/pipeline/#preprocessing-in-mzmine","title":"Preprocessing in MZMine","text":"<ul> <li>mzmine_params: MZMine parameters</li> <li>mzmine_threads: MZMine number of threads</li> </ul>"},{"location":"metpipe/pipeline/#library-used-for-preprocessing","title":"Library used for preprocessing","text":"<ul> <li>path_lib_istd_pos/path_lib_istd_neg: path to in-house library including internal standards</li> <li>path_spectral_lib: path to spectral library</li> </ul>"},{"location":"metpipe/pipeline/#post-processing-parameters","title":"Post-processing parameters","text":""},{"location":"metpipe/pipeline/#filtering","title":"Filtering","text":"<ul> <li>mean.filter: mean filter threshold, e.g. 2*BL &lt; PO (meaning features with mean intensity in PO samples more than two times the mean intensity in BL samples will be kept).  </li> </ul> <pre><code>- - '2'  \n- BL  \n- &lt;  \n- PO  \n</code></pre> <ul> <li>rsd.filter: RSD filter threshold, e.g. 30 (meaning features with RSD less than 30% in PO samples will be kept).  </li> </ul> <pre><code>- - '1'\n- PO\n- &lt;\n- '0.3'\n</code></pre> <ul> <li>rt.range: retention time range to keep, e.g. (0, 30) (meaning features with retention time between 0 and 30 minutes will be kept).  </li> </ul> <pre><code>- 0\n- 30\n</code></pre> <ul> <li>filter_by_missing_feature_pct: filter features by missing percentage across all specific type of samples, e.g. 50 (meaning features with less than 50% missing values across all specific type of samples will be kept).</li> <li>filter_by_missing_sample_type: sample types used for missing value filtering, e.g. PO (meaning only PO samples will be used for missing value filtering).</li> <li>outliers.sample: sample names to be considered as outliers and removed from further processing, e.g. BL_1_3, PO_1_1.  </li> </ul> <pre><code>- BL_1_3\n- PO_1_1\n</code></pre> <ul> <li>clean.po.sample.to.use: sample types used for cleaning duplicate features, e.g. PO (meaning only PO samples will be used for cleaning duplicate features)</li> </ul>"},{"location":"metpipe/pipeline/#imputation","title":"Imputation","text":"<ul> <li>impute.method.sample: imputation method for sample missing values, e.g. HF, LoD, median, min, mean, knn.  </li> <li>impute.method.is: imputation method for internal standard missing values</li> </ul>"},{"location":"metpipe/pipeline/#normalization","title":"Normalization","text":"<ul> <li>norm.po.sample.to.use: sample types used for normalization, e.g. PO</li> <li>norm.method: normalization method, e.g. bestis, low_cv, median, sum, pqn, loess, combat, limma.   </li> </ul> <pre><code>- bestis\n- low_cv\n- sum\n- median\n</code></pre> <ul> <li>sample.type.keep: sample types to keep for further analysis after normalization, e.g. PO, NIST.  </li> </ul> <pre><code>- PO\n- NIST\n</code></pre>"},{"location":"metpipe/pipeline/#merge-positive-and-negative-mode-data","title":"Merge positive and negative mode data","text":"<ul> <li>final.norm: final normalization method to choose</li> <li>final.eval.sample.to.use: sample types used for cleaning duplicate features in the final merged data.</li> <li>final.sample.type.keep: sample types to keep for further analysis in the final merged data.  </li> </ul> <pre><code>- PO\n- NIST\n</code></pre>"},{"location":"metpipe/pipeline/#metadata","title":"Metadata","text":"<ul> <li>raw_files_pos: list of raw/mzML files in positive mode</li> <li>meta_pos: metadata positive mode data, e.g. Sample.type, Run.seq</li> <li>raw_files_neg: list of raw/mzML files in negative mode</li> <li>meta_neg: metadata hegative mode data, e.g. Sample.type, Run.seq</li> </ul>"},{"location":"metpipe/pipeline/#run-workflow","title":"Run workflow","text":"<p>To execute the full processing workflow, run the following command in powershell:</p> <pre><code>docker run -it `\n    -v E:/:/mnt/e `\n    qiangao/metpipe:0.5 `\n    Rscript /wd/run_workflow.R --raw \"/mnt/e/Projects/MP_workshop/peaktable/PL_LIPS_PASEF/config.yml\"\n</code></pre> <p>In this step, the <code>run_workflow.R</code> script is executed, using the <code>config.yml</code> file generated in the previous step. Adjust the paths as necessary for your system. The processing logs and result files will be written to the output directory specified in config.yml.</p>"},{"location":"metpipe/pipeline/#re-run-workflow","title":"Re-run workflow","text":""},{"location":"metpipe/pipeline/#re-run-mzmine-preprocessing-with-modified-parameters","title":"Re-run MZMine preprocessing with modified parameters","text":"<p>In the initial run of the workflow, default MZMine processing parameters are generated and saved in pos/neg folders inside the result folder. To modify and re-run MZMine preprocessing with custom parameters, edit the <code>mzmine_parameters_pos/neg.mzbatch</code> files in the respective pos/neg folders. After making the desired changes, remove the preprocessing output files in the previous run in pos/neg folder, and execute the run_workflow.ps1 script again.</p> <p>To better understand the MZMine parameters, the .mzbatch files can be opened in MZMine GUI (Project -&gt; Batch mode -&gt; Load). After editing and saving the parameters in MZMine GUI, export the updated parameters back to the .mzbatch files. Then, re-run the run_workflow.R script as shown above.</p>"},{"location":"metpipe/pipeline/#re-run-downstream-processing-with-modified-parameters","title":"Re-run downstream processing with modified parameters","text":"<p>To re-run the downstream processing with modified parameters (e.g., filtering thresholds, normalization methods), edit the <code>config.yml</code> file generated in the prepare_workflow step. After making the desired changes, execute the run_workflow.ps1 script again.</p>"},{"location":"metpipe/pipeline/#output","title":"Output","text":"<p>All the output files will be saved in the specified result folder (path_result in config.yml). The main output files include:</p> <ul> <li><code>pos/neg folders</code>: MZMine preprocessing output files for positive and negative mode data, respectively.</li> <li><code>qc_report.html</code>: Quality control report summarizing the internal standards in all samples. Based on prerpocessed peaktable before post-processing.</li> <li><code>peaktable_pos/neg.xlsx</code>: Preprocessed peaktable, before post-processing. </li> <li><code>peaktable_istd_pos/neg.xlsx</code>: Preprocessed peaktable including only internal standards.</li> <li><code>post_processing.html</code>: Documentation of post-processing steps applied to the data, including filtering, imputation, normalization, and merging of positive and negative mode data.</li> <li><code>evaluation.html</code>: Evaluation report summarizing the missing distribution, qc of internal standard, normalization comparison, batch evaluation and annotation summary. Based on post-processed peaktable.</li> <li><code>datatable.xlsx</code>: Final processed data table after post-processing, including merged positive and negative mode data and feature annotations and database mappings.</li> </ul>"},{"location":"metpipe/pipeline/#one-step-command","title":"One step command","text":"<p>To run the entire workflow in a single step, you can use the following command in powershell:</p> <pre><code>docker run -it `\n    -v E:/:/mnt/e `\n    qiangao/metpipe:0.5 `\n    Rscript /wd/prepare_run_workflow.R --raw \"/mnt/e/Projects/MP_workshop/raw/PL_LIPS_PASEF\"\n</code></pre>"},{"location":"metpipe/pipeline/#guidance-for-non-standardized-workflow","title":"Guidance for non-standardized workflow","text":"<p>If your data contains more qc types than the standardized ones (e.g. BL, PO, NIST, CP), you could prepare non-standardized workflow by running the following command in powershell: </p> <pre><code>docker run -it `\n    -v E:/:/mnt/e `\n    qiangao/metpipe:0.5 `\n    Rscript /wd/prepare_workflow.R --raw \"/mnt/e/Projects/MP_workshop/raw/PL_LIPS_PASEF\" --qc \"BL,PO,NIST,CP,ABC,DEF\"\n</code></pre> <p>--qc argument allows users to specify all qc types included in the data, separated by commas. After running this command, a default <code>config.yml</code> file will be generated in the output directory. Check and edit the parameters in <code>config.yml</code> as needed before running the <code>run_workflow.R</code> script.</p>"},{"location":"metpipe/setup_input_folder/","title":"Standardized workflow","text":"<p>The only input needed is the subfolder name where the raw/mzML data is stored (e.g. PL_LIPS_PASEF). The subfolder naming should follow the standard naming convention as shown in the figure below (i.e. Matrix_Assay_method). The data file naming should follow the standard naming convention (i.e. Project ID_Assay Method Polarity_Date_Run Seq_Sample Name_Extraction Replicate_Technical Replicate, e.g. MP000_LIPSPASEFp_09092025_004_BL_1_1).</p> <p></p>"},{"location":"metpipe/setup_input_folder/#supported-assays-and-methods","title":"Supported assays and methods","text":"<p>The pipeline current supports the following assays and methods:  </p> <ul> <li>Assays: Lipidomics (LIPS, LIPL), Metabolomics (RP, zHILIC)</li> <li>Methods: FDDA*, PASEF</li> </ul> <p>* Currently FDDA files need to be converted to mzML format before processing.</p>"},{"location":"metpipe/setup_input_folder/#default-library-setting","title":"Default library setting","text":"<p>The pipeline will automatically link the in-house MS1/MS2 libraries and public spectral libraries based on the assay detected from the subfolder name. The default library settings are as follows:</p>"},{"location":"metpipe/setup_input_folder/#lipidomics-lips-lipl","title":"Lipidomics (LIPS, LIPL):","text":"<ul> <li>In-house MS1 library (internal standards only)<ul> <li>MP_ISTD_mzmine_Lipidomics_4min_pos_20250507.csv</li> <li>MP_ISTD_mzmine_Lipidomics_4min_neg_20250507.csv</li> <li>MP_ISTD_mzmine_Lipidomics_10min_pos_20250507.csv</li> <li>MP_ISTD_mzmine_Lipidomics_10min_neg_20250507.csv</li> </ul> </li> <li>Public spectral library<ul> <li>MoNA-export-LipidBlast_2022.msp</li> </ul> </li> </ul>"},{"location":"metpipe/setup_input_folder/#metabolomics-zhilic-rp","title":"Metabolomics (zHILIC, RP):","text":"<ul> <li>In-house metabolite library (internal standards and standards)<ul> <li>MP_library_mzmine_zHILIC_neg_20251023.csv</li> </ul> </li> <li>In-house MS2 library<ul> <li>MP_metabolomics_RP_60.msp</li> </ul> </li> <li>Public spectral library<ul> <li>MSMS-Public_experimentspectra-VS19.msp</li> </ul> </li> </ul>"},{"location":"metpipe/setup_input_folder/#example-for-plasma-lipidomics-analysis-with-short-pasef-method","title":"Example for plasma lipidomics analysis with short PASEF method","text":""},{"location":"metpipe/setup_input_folder/#folder-structure","title":"Folder structure","text":"<pre><code>- Projects\n  - MP001\n    - raw\n      - PL_LIPS_PASEF\n        - pos\n        - neg\n</code></pre> <p>The 'PASEF' in the subfolder name indicates the acquisition method used. This means in pos/neg folders, all the data files should be PASEF data files.   </p>"},{"location":"metpipe/setup_input_folder/#example-for-plasma-metabolomics-analysis-with-zhilic-column-with-full-scan-and-dda-acquisition","title":"Example for plasma metabolomics analysis with zHILIC column with full scan and DDA acquisition","text":""},{"location":"metpipe/setup_input_folder/#folder-structure_1","title":"Folder structure","text":"<pre><code>- Projects\n  - MP001\n    - mzML\n      - PL_zHILIC_FDDA\n        - neg\n</code></pre> <p>The 'FDDA' in the subfolder name indicates the acquisition method used. This means in neg folder, all the data files should be either full scan or DDA data files, and both types need to exist. For DDA data files, the file name should contain 'DDA' in the Assay method part (e.g. MP001_zHILICDDAn_15102025_000_PO_1_1).</p>"},{"location":"workflow/design_data_conversion/","title":"Data conversion","text":"<p>Many LC\u2013MS instruments generate data in vendor-specific formats (e.g., Bruker <code>.d</code>, Thermo <code>.raw</code>). These formats are sometimes not directly supported by open-source tools used in metabolomics/lipidomics workflows. A standard first step is therefore to convert raw files into an open format (most commonly <code>mzML</code>) while preserving the information needed for downstream feature detection and QC.</p>"},{"location":"workflow/design_data_conversion/#msconvert-proteowizard","title":"MSConvert (ProteoWizard)","text":"<p>MSConvert from ProteoWizard supports conversion for AB SCIEX, Agilent, Bruker, Shimadzu, Thermo Scientific, and Waters data. It can also apply optional filters (e.g., vendor peak picking, compression) during conversion.</p>"},{"location":"workflow/design_data_conversion/#profile-vs-centroid-data","title":"Profile vs centroid data","text":"<ul> <li>Profile data contain the raw instrument signal and are generally the most information-rich.</li> <li>Centroid data store only detected peaks and can be much smaller and faster to process.</li> </ul> <p>Centroiding can be helpful for large studies, but it can also affect: - low-intensity features, - peak shapes, - downstream peak detection/quantification depending on the software.</p>"},{"location":"workflow/design_data_conversion/#recommended-conversion-settings","title":"Recommended conversion settings","text":"<p>These settings are typical for high-resolution MS1 (and MS/MS when present). Exact choices can be instrument- and workflow-dependent.</p> <ul> <li>m/z precision: use 64-bit m/z to retain mass accuracy.</li> <li>Compression: optional, but usually recommended to reduce storage. If available, prefer zlib compression (widely supported). Numpress compression can yield smaller files but may not be supported by all tools. If you require no precision loss and maximum compatibility, use zlib or no compression.</li> <li>Vendor peak picking (centroiding):</li> <li>If centroiding during conversion, use the vendor peak picking option.</li> <li>Make sure Peak Picking is the first filter in the filter list (ProteoWizard applies filters in order; placing it later can yield unexpected results).</li> <li>Noise filtering (optional): a threshold filter can reduce file size and noise, but be cautious\u2014over-filtering can remove real low-abundance signals.</li> </ul> <p></p>"},{"location":"workflow/design_data_conversion/#practical-checklist","title":"Practical checklist","text":"<p>After conversion, verify:</p> <ul> <li>File count matches the number of injections (including blanks/QCs).</li> <li>Polarity handling is as expected (some instruments/methods generate separate files or separate scans).</li> <li>MS1/MS2 levels are present as expected (if MS/MS was acquired).</li> <li>RT and m/z ranges look plausible in a quick viewer (e.g., vendor software, Skyline, MZmine, or MSConvert preview).</li> </ul>"},{"location":"workflow/design_data_structure/","title":"LC\u2013MS data structure","text":"<p>LC\u2013MS(/MS) raw data can be viewed as a time-ordered series of mass spectra acquired over a chromatographic separation. Each spectrum contains signal intensities measured across mass-to-charge ratio (m/z) at a specific retention time (RT). Together, the raw data form a three-dimensional data space:</p> <ul> <li>m/z</li> <li>RT</li> <li>Intensity (abundance)</li> </ul> <p></p> <p>A common visualization is the total ion chromatogram (TIC), which summarizes each spectrum into a single intensity value and plots it versus RT. At any RT point in the TIC, the corresponding mass spectrum can be inspected to see how signal is distributed across m/z.</p>"},{"location":"workflow/design_data_structure/#nomenclature","title":"Nomenclature","text":"<ul> <li>Chromatogram: Intensity vs RT (constructed by summarizing or selecting m/z signals across spectra).</li> <li> <p>Spectrum: Intensity vs m/z for a single acquisition time point (a \u201cscan\u201d) at a specific RT.</p> </li> <li> <p>Total ion chromatogram (TIC): For each RT, the sum of intensities across all m/z in the spectrum (optionally within an m/z range).</p> </li> <li> <p>Base peak chromatogram (BPC): For each RT, the maximum intensity observed in the spectrum (the base peak), plotted vs RT.</p> </li> <li> <p>Total ion spectrum (TIS): The sum (or average) of intensities across spectra over an RT range, yielding a single spectrum summarizing that region.</p> </li> <li> <p>Extracted ion chromatogram (XIC): Intensity vs RT for a selected m/z window (e.g., a narrow range around an expected m/z). XICs are used to inspect candidate compounds/features and assess peak shape and RT.</p> </li> <li> <p>Isotope trace: The chromatographic trace for one isotopologue (e.g., M, M+1) of a compound/feature within a defined m/z tolerance.</p> </li> <li>Isotopic envelope: The set of isotope traces (M, M+1, M+2, \u2026) associated with the same compound/feature (and charge state, if applicable).</li> </ul>"},{"location":"workflow/design_data_structure/#ms-levels-ms1-vs-ms2","title":"MS levels (MS1 vs MS2)","text":"<p>Depending on the acquisition method, files may contain:</p> <ul> <li>MS1 spectra: survey scans used for feature detection and quantification (typical for untargeted workflows).</li> <li>MS2 (MS/MS) spectra: fragmentation spectra used primarily for annotation/identification (library matching, structural inference).</li> </ul>"},{"location":"workflow/design_data_structure/#data-representation-profile-vs-centroid","title":"Data representation: profile vs centroid","text":"<p>LC\u2013MS data are commonly stored in two forms: profile and centroid.</p> <p></p> <ul> <li> <p>Profile data: the instrument-reported signal over m/z, represented as a dense series of points.  </p> <ul> <li>Pros: most information-rich; preserves peak shape.  </li> <li>Cons: larger files; heavier computation.</li> </ul> </li> <li> <p>Centroid data: a reduced representation where each peak is summarized by (m/z, intensity) at local maxima (centroided peaks).  </p> <ul> <li>Pros: smaller files; faster processing.  </li> <li>Cons: centroiding choices can affect low-intensity signals and peak representation.</li> </ul> </li> </ul> <p>Practical implication: whether to work with profile or centroid depends on the instrument, conversion settings, and downstream software. If you centroid during conversion, document the method (e.g., vendor peak picking) and keep vendor raw files archived.</p>"},{"location":"workflow/design_data_structure/#ion-mobility-optional-4th-dimension","title":"Ion mobility (optional 4th dimension)","text":"<p>Ion mobility spectrometry (IMS) adds an additional separation based on ion size/shape/charge, creating a four-dimensional structure:</p> <ul> <li>m/z</li> <li>RT</li> <li>Ion mobility</li> <li>Intensity</li> </ul> <p>Ion mobility may be reported as:</p> <ul> <li>Drift time (DT): instrument-specific time coordinate</li> <li>Inverse reduced mobility (1/K0): mobility metric that is more comparable across settings</li> <li>Collision cross section (CCS): a physicochemical property related to ion shape/size; often used for identification support</li> </ul> <p>IMS can improve separation of isomers and reduce spectral complexity, but it also increases data volume and may require IMS-aware processing tools.</p>"},{"location":"workflow/design_intro/","title":"Study design &amp; data organization","text":"<p>Good analysis starts before data acquisition. This section summarizes practical guidance for designing an LC\u2013MS metabolomics/lipidomics study and introduces the data conversion and data structure of LC-MS(/MS) data.</p>"},{"location":"workflow/design_intro/#overview","title":"Overview","text":"<ul> <li>Randomization: strategies to minimize confounding technical variation with biological groups.</li> <li>QC samples setup: recommended QC samples (blanks, pooled QCs, internal standards) and their placement in the acquisition sequence.</li> <li>Data conversion: converting vendor raw files to open formats (e.g., mzML).</li> <li>LC-MS data structure: understanding the organization and representation of LC-MS(/MS) data, including MS levels, profile vs centroid data, and optional ion mobility.</li> </ul>"},{"location":"workflow/design_qc_setup/","title":"Quality control setup","text":"<p>Quality control (QC) is essential for producing reliable and comparable LC\u2013MS metabolomics/lipidomics data. QC samples and procedures are used to (1) detect problems early (e.g., contamination, carryover, retention-time drift, sensitivity loss), (2) quantify analytical variability, and (3) support downstream steps such as filtering, normalization, and batch-effect handling.</p> <p>Many sources of unwanted variation can affect measured intensities, including pre-analytical factors (collection, storage, extraction, plate effects) and instrumental factors (mass accuracy, ion-source condition, chromatographic aging, temperature changes). In long sequences and multi-day studies, these effects often appear as systematic trends with run order and/or differences between batches. A well-planned QC scheme makes these patterns observable and, where appropriate, correctable.</p>"},{"location":"workflow/design_qc_setup/#qc-sample-types","title":"QC sample types","text":"QC type How it is prepared Primary purpose Typical use Blank (BL) Processed with the same solvents/chemicals/labware as study samples but without biological material (e.g., water or extraction solvent) Identify contaminants, background, and carryover Filter features present in blanks; diagnose contamination over time Pooled QC (PO) Equal (or representative) aliquots from many study samples mixed into one pool Monitor instrument stability and analytical precision across the run Inject regularly to assess drift; compute RSD; support QC-based normalization Dilution series (DS) Serial dilutions of a pooled QC or reference material Assess linearity and dynamic range of features/targets Optional; supports linearity checks and response factor estimation Reference material (e.g., NIST SRM) Certified/standard reference material (e.g., NIST SRM 1950 for human plasma metabolomics) Benchmark comparability and long-term performance Periodic injections; compare coverage/response across batches and time"},{"location":"workflow/design_qc_setup/#internal-standards-is","title":"Internal standards (IS)","text":"<p>Internal standards are typically spiked into study samples and QC samples (as appropriate for the method) and used to monitor:</p> <ul> <li>Retention time (RT) stability</li> <li>Mass accuracy (m/z) stability</li> <li>Signal response / sensitivity drift</li> </ul> <p>IS behavior is also used for diagnostic plots and, in some workflows, to support normalization or correction steps (method-dependent).</p>"},{"location":"workflow/design_qc_setup/#recommended-placement-in-the-analytical-sequence","title":"Recommended placement in the analytical sequence","text":"<p>A common and defensible LC\u2013MS sequence structure is:</p> <ol> <li>System suitability / conditioning injections (method-specific; often includes pooled QC, blanks, dilution series and reference material)</li> <li>Study samples with regular QC injections (e.g., pooled QC every 5\u201310 injections)</li> <li>Blanks placed to monitor contamination and carryover (e.g., after high-concentration samples or at regular intervals)</li> <li>End-of-run pooled QCs, blanks, reference materials and dilution series</li> </ol> <p>For multi-day or multi-batch studies, ensure that each batch contains:</p> <ul> <li>A balanced mix of biological groups (design principle)</li> <li>The same QC scheme (pooled QC frequency, blanks, dilution series and reference material)</li> </ul>"},{"location":"workflow/design_qc_setup/#scope-note","title":"Scope note","text":"<p>The QC principles above apply to both untargeted and targeted LC\u2013MS workflows. Untargeted pipelines often rely more heavily on pooled-QC-driven diagnostics and feature filtering, while targeted assays may emphasize calibration/QC levels and acceptance criteria defined by the assay SOP.</p>"},{"location":"workflow/design_randomization/","title":"Randomization","text":"<p>Randomization is a core element of experimental design. In LC\u2013MS metabolomics/lipidomics it helps ensure that systematic technical effects (e.g., instrument drift, column aging, carryover, temperature changes, operator effects) do not become confounded with biology (e.g., case/control, treatment groups, time points).</p> <p>In practice, randomization is usually applied at multiple stages:</p> <ul> <li>Sample preparation order (extraction/derivatization/dilution)</li> <li>Injection/run order (sequence on the instrument)</li> <li>Batch allocation (if samples must be measured across days, columns, instruments, or methods)</li> </ul> <p>Samples should be randomized during both preparation and acquisition. When necessary, allocate to batches in a way that balances groups across batches. The goal is to ensure that technical variation is distributed across groups rather than systematically aligned with them. </p>"},{"location":"workflow/design_randomization/#why-randomize-in-lcms","title":"Why randomize in LC\u2013MS?","text":"<p>Even with stable instrumentation, LC\u2013MS data typically show time-dependent and batch-dependent variation. If one biological group is injected earlier and another later, differences can reflect run-order effects rather than true biology.</p> <p>Randomization aims to:</p> <ul> <li>Distribute unknown nuisance variation across groups</li> <li>Reduce bias in estimates of group differences</li> <li>Improve robustness of downstream steps such as normalization and differential analysis</li> </ul> <p></p>"},{"location":"workflow/design_randomization/#types-of-randomization","title":"Types of randomization","text":""},{"location":"workflow/design_randomization/#complete-randomization","title":"Complete randomization","text":"<p>In complete randomization, every sample has the same probability of being assigned to any position (e.g., injection index 1\u2026N).</p> <p>Use when:</p> <ul> <li>You have a single batch (or batches are negligible)</li> <li>You have no strong constraints (e.g., few special samples, minimal carryover concerns)</li> <li>Group sizes are reasonably balanced</li> </ul> <p>Limitations:</p> <ul> <li>By chance, a group can still cluster early/late in the run, especially for small studies.</li> <li>It does not guarantee balance across time windows.</li> </ul> <p></p>"},{"location":"workflow/design_randomization/#block-randomization","title":"Block randomization","text":"<p>Block randomization means constructing the injection sequence as a series of small blocks, where each block contains a balanced composition of samples with respect to:</p> <ul> <li>the primary biological groups (e.g., case/control, dose levels), and</li> <li>selected covariates (i.e., strata such as sex, site, acquisition day, plate), when applicable.</li> </ul> <p>You then randomize the order within each block and often also randomize block order. This design distributes samples evenly across the run, reducing the risk that run-order drift becomes confounded with biology.</p> <p>Use when:</p> <ul> <li>You expect drift over time (common in long sequences)</li> <li>You want each part of the run to contain a similar composition of groups</li> <li>You have multiple plates or preparation days and want balance within each</li> </ul> <p>Typical ways to define blocks:</p> <ul> <li>Fixed-size injection windows (e.g., blocks of 8, 12, or 16 injections), constructed to be balanced by group/strata</li> <li>Plate- or day-based blocks (when acquisition is naturally segmented), again constructed to be balanced by group/strata</li> </ul> <p></p> <p></p> <p>Note for repeated measures / paired samples: </p> <p>If multiple samples originate from the same subject (e.g., longitudinal time points, matched tissues, or technical replicates), keep them in the same block (and ideally the same analytical batch) whenever possible. This reduces within-subject differences driven by drift across distant parts of the run and preserves the interpretability of paired/within-subject comparisons.</p>"},{"location":"workflow/design_randomization/#batch-allocation","title":"Batch allocation","text":"<p>When the sequence is too long for a single batch, or when samples arrive in multiple waves, you may need to run multiple batches. In this case, batch design is critical to ensure that batch effects do not become confounded with biology.</p> <p>A \u201cbatch\u201d can mean different things in practice:</p> <ul> <li>Preparation batch: same extraction day/operator/reagent lot/plate</li> <li>Analytical batch: same day/column/instrument/method settings</li> </ul> <p>The key rule is that do not let batch membership become a proxy for biological group.</p> <p>Recommendations:</p> <ul> <li>Allocate samples so each batch contains a balanced representation of study groups.</li> <li>Spread independent biological replicates across batches when feasible (to avoid confounding biological effects with batch and to make batch effects estimable).</li> <li>If you must run a group in a separate batch (e.g., samples arrive later), document it and plan for explicit batch modeling.</li> </ul> <p></p> <p>References:</p> <ol> <li>Burger, Bram, Marc Vaudel, and Harald Barsnes. \"Importance of block randomization when designing proteomics experiments.\" Journal of proteome research 20.1 (2020): 122-128.</li> </ol>"},{"location":"workflow/introduction/","title":"Introduction","text":"<p>This section describes recommended computational best practices for LC\u2013MS-based metabolomics and lipidomics data processing and statistical analysis. It is not a software manual, but rather a set of guidelines and principles to help researchers make informed decisions at each step of the workflow.</p>"},{"location":"workflow/introduction/#workflow-overview","title":"Workflow overview","text":"<ul> <li>Start with study design &amp; data organization to ensure your acquisition plan, QC strategy, and metadata are consistent and analysis-ready.</li> <li>Then follow either the untargeted or targeted processing track to convert raw data into a clean, normalized data table of features/targets \u00d7 samples.</li> <li>Finally, perform statistical analysis and interpretation to extract biological insights.</li> </ul>"},{"location":"workflow/introduction/#study-design-data-organization","title":"Study design &amp; data organization","text":"<ul> <li>Introduction</li> <li>Randomization</li> <li>Quality control setup</li> <li>Data conversion</li> <li>LC-MS data structure</li> </ul>"},{"location":"workflow/introduction/#untargeted-data-processing","title":"Untargeted data processing","text":"<ul> <li>Introduction</li> <li>Preprocessing</li> <li>Identification<ul> <li>General</li> <li>Lipid annotation</li> </ul> </li> <li>Quality control</li> <li>Post-processing<ul> <li>Peak cleaning</li> <li>Missing imputation</li> <li>Normalization</li> <li>Merge peaklist</li> </ul> </li> </ul>"},{"location":"workflow/multivariate/","title":"Multivariate Analysis","text":"<p>Multivariate analysis examines multiple variables (features) simultaneously to uncover patterns, groupings, or predictive relationships in complex datasets. In metabolomics and lipidomics, these methods are essential for data exploration, classification, and biomarker discovery.</p>"},{"location":"workflow/multivariate/#why-use-multivariate-analysis","title":"Why Use Multivariate Analysis?","text":"<ul> <li>Biological systems are complex, with many correlated variables.</li> <li>Multivariate methods can:<ul> <li>Reveal hidden structure (e.g., clusters, trends)</li> <li>Reduce dimensionality for visualization</li> <li>Build predictive models for classification or regression</li> <li>Identify combinations of features that best explain group differences</li> </ul> </li> </ul>"},{"location":"workflow/multivariate/#unsupervised-dimensionality-reduction","title":"Unsupervised Dimensionality Reduction","text":"<p>Unsupervised methods do not use group labels and are used for data exploration and visualization.</p> <ul> <li>Principal Component Analysis (PCA):<ul> <li>Reduces data to a few principal components that capture the most variance.</li> <li>Useful for visualizing sample clustering, detecting outliers, and assessing batch effects.</li> <li>Assumes linear relationships among variables.</li> </ul> </li> <li>t-SNE (t-distributed Stochastic Neighbor Embedding):<ul> <li>Non-linear method for visualizing high-dimensional data in 2D or 3D.</li> <li>Preserves local structure (similarity of neighbors) but not global distances.</li> <li>Sensitive to parameter choices (perplexity, learning rate).</li> </ul> </li> <li>UMAP (Uniform Manifold Approximation and Projection):<ul> <li>Non-linear, scalable method for dimensionality reduction.</li> <li>Preserves both local and some global structure; often faster than t-SNE.</li> <li>Useful for large datasets and complex clustering patterns.</li> </ul> </li> </ul>"},{"location":"workflow/multivariate/#supervised-methods","title":"Supervised Methods","text":"<p>Supervised methods use known group labels (e.g., case/control) to build models that discriminate between groups or predict outcomes.</p> <ul> <li>Partial Least Squares Discriminant Analysis (PLS-DA):<ul> <li>Projects data into a new space that maximizes separation between groups.</li> <li>Useful for identifying features that contribute most to group differences.</li> <li>Risk of overfitting; always validate with cross-validation or permutation tests.</li> </ul> </li> <li>Orthogonal PLS-DA (OPLS-DA):<ul> <li>Separates predictive variation (related to group separation) from orthogonal (unrelated) variation.</li> <li>Can improve interpretability of PLS-DA models.</li> </ul> </li> <li>ASCA (ANOVA\u2013Simultaneous Component Analysis):<ul> <li>Combines ANOVA and PCA to analyze effects of experimental factors and their interactions.</li> <li>Useful for complex experimental designs (e.g., time series, factorial designs).</li> </ul> </li> <li>MEBA (Multivariate Empirical Bayes Analysis):<ul> <li>Designed for time series data; identifies features with significant temporal changes across conditions.</li> </ul> </li> </ul>"},{"location":"workflow/multivariate/#clustering-methods","title":"Clustering Methods","text":"<p>Clustering groups samples or features based on similarity, revealing natural groupings in the data.</p> <ul> <li>Hierarchical clustering:<ul> <li>Builds a tree (dendrogram) of sample or feature relationships.</li> <li>Often visualized with heatmaps; useful for exploring patterns and subgroups.</li> </ul> </li> <li>K-means clustering:<ul> <li>Partitions samples into a predefined number of clusters based on similarity.</li> <li>Sensitive to initial cluster centers and number of clusters chosen.</li> </ul> </li> <li>Density-based clustering (e.g., DBSCAN):<ul> <li>Identifies clusters of arbitrary shape based on point density.</li> <li>Can detect noise and outliers as unclustered points.</li> </ul> </li> </ul>"},{"location":"workflow/multivariate/#machine-learning-methods","title":"Machine Learning Methods","text":"<p>Machine learning approaches can be used for classification, regression, and feature selection in high-dimensional omics data.</p> <ul> <li>Random Forests:<ul> <li>Ensemble method for classification or regression.</li> <li>Provides feature importance scores; robust to overfitting.</li> </ul> </li> <li>Support Vector Machines (SVM):<ul> <li>Finds the optimal separating hyperplane for classification or regression.</li> <li>Effective in high-dimensional spaces; kernel trick allows for non-linear boundaries.</li> </ul> </li> <li>Elastic Net regression:<ul> <li>Regularized regression combining L1 (lasso) and L2 (ridge) penalties.</li> <li>Useful for feature selection and handling correlated predictors.</li> </ul> </li> </ul>"},{"location":"workflow/pathway/","title":"Pathway analysis","text":"<p>Pathway analysis in metabolomics/lipidomics requires extra care because metabolite coverage, mapping, and pathway overlap behave very differently than gene-centric data. Pathway results should usually be interpreted as pathway-consistent signals (context), not as definitive proof that a specific pathway is \u201cactivated\u201d.</p>"},{"location":"workflow/pathway/#why-pathway-analysis-is-different-for-metabolites-vs-genes","title":"Why pathway analysis is different for metabolites (vs genes)","text":"<ul> <li>Many-to-many mapping is common: one MS feature can map to multiple candidate metabolites; one metabolite often belongs to multiple pathways.</li> <li>Pathway \u201centanglement\u201d: pathways overlap heavily in metabolite space; ubiquitous \u201chub\u201d metabolites (e.g., ATP, NAD(H), acetyl-CoA; also common lipid backbones) can make many pathways appear significant.</li> <li>Sparse and biased coverage: especially for targeted metabolomics (and many lipidomics panels), only a subset of the metabolome is measured and this subset is platform/method dependent.</li> <li>Identification uncertainty propagates: pathway results are only as good as the metabolite IDs and mapping rules; isomer ambiguity can directly translate into pathway ambiguity.</li> <li>Directionality is less straightforward: unlike transcript abundance, metabolite abundance reflects production/consumption, transport, compartmentation, and regulation; a \u201cpathway change\u201d is rarely captured by one metabolite.</li> </ul> <p>Practical implication: pathway tools are best used to summarize patterns and generate hypotheses, then followed by targeted validation.</p>"},{"location":"workflow/pathway/#enrichment-analysis","title":"Enrichment analysis","text":"<p>Enrichment (metabolite set enrichment analysis, MSEA) tests whether a set of metabolites associated with a pathway or chemical class shows a stronger signal than expected.</p> <p>Typical inputs (depending on method/tool):</p> <ul> <li>a list of significant metabolites (or annotated features)</li> <li>a ranked list (e.g., by statistic)</li> <li>a quantitative table (samples \u00d7 metabolites)</li> </ul> <p>Key requirement: define the correct background/universe.</p> <ul> <li>Unlike transcriptomics, metabolomics/lipidomics usually does not measure \u201call possible metabolites\u201d. Many metabolites in a database have zero probability of being observed in your assay.</li> <li>If the tool supports it, provide a reference metabolome (all metabolites measurable by your platform/method) or use the set of metabolites that passed QC as the universe.</li> </ul> <p>Interpretation:</p> <ul> <li>Enrichment results are sensitive to shared metabolites across pathways. Expect related pathways to co-appear.</li> <li>Prefer statements like \u201csignals consistent with \u2026\u201d rather than declaring a specific pathway is activated.</li> </ul>"},{"location":"workflow/pathway/#pathway-analysis_1","title":"Pathway analysis","text":"<p>Metabolomics pathway analysis typically combines:</p> <ul> <li>set-level statistics (enrichment / over-representation / rank-based)</li> <li>topology-aware scoring (optionally weighting metabolites by their role/location in the pathway network)</li> </ul> <p>Use cases:</p> <ul> <li>Summarize which biological processes are most consistent with observed metabolite changes.</li> <li>Prioritize pathways for follow-up (targeted measurement, flux, enzyme assays, genetic perturbations).</li> </ul> <p>Common pitfalls:</p> <ul> <li>Treating the pathway database as ground truth: pathway definitions are curated and incomplete and can differ by resource/version.</li> <li>Over-interpreting \u201ctop pathways\u201d when only a few shared metabolites drive significance.</li> <li>Ignoring identification confidence (putative IDs and ambiguous mapping inflate false specificity).</li> </ul>"},{"location":"workflow/pathway/#network-analysis","title":"Network analysis","text":"<p>Network analysis places metabolites (and optionally genes/enzymes) into a graph context (e.g., global metabolic networks or curated association networks) to support exploration.</p> <p>Use it to:</p> <ul> <li>visualize connectivity among measured metabolites</li> <li>identify clusters/modules that may reflect shared biochemistry</li> <li>inspect whether \u201csignificant pathways\u201d are driven by a few hub metabolites</li> </ul> <p>Network visualization is best treated as interpretation support, not confirmation.</p>"},{"location":"workflow/pathway/#common-tools-examples","title":"Common tools (examples)","text":"<p>The choice of tool depends on whether you start from confident metabolite IDs, putative annotations, or MS features. Regardless of tool, apply the same principles above (mapping transparency, appropriate background, and conservative interpretation).</p> <ul> <li>MetaboAnalyst: enrichment (MSEA) and pathway analysis (enrichment + topology), plus visualization.</li> <li>MetaboAnalystR: R interface enabling scripted, reproducible analyses using MetaboAnalyst methods.</li> <li>mummichog: feature-to-pathway activity prediction for untargeted LC\u2013MS when metabolite IDs are incomplete.</li> <li>GSEA-style / rank-based methods (implemented in multiple toolkits): use ranked metabolite lists to reduce dependence on hard cutoffs.</li> <li>ChemRICH: chemical similarity enrichment as an alternative to pathway databases (useful when pathway mapping is sparse/ambiguous).</li> <li>Cytoscape (with pathway/network apps): flexible network visualization and integration of metabolites/genes; interpretation depends on the chosen network resource.</li> </ul> <p>Lipid-focused pathway/context tools vary by database coverage. Common choices include lipid class enrichment and pathway-context approaches provided by platform-specific tools and lipid resources (report the database/resource and version used).</p>"},{"location":"workflow/pathway/#recommended-workflow-practical-and-defensible","title":"Recommended workflow (practical and defensible)","text":"<ol> <li> <p>Decide what you want to claim</p> <ul> <li>Hypothesis generation and biological context (typical)</li> <li>Narrow pathway support for a specific mechanism (requires stronger evidence)</li> </ul> </li> <li> <p>Define inputs and identification confidence</p> <ul> <li>Prefer identified metabolites and record confidence levels.</li> <li>If using putative annotations/features, state this clearly and keep pathway claims exploratory.</li> </ul> </li> <li> <p>Make mapping rules explicit</p> <ul> <li>Report which identifiers you use (HMDB/KEGG/ChEBI/InChIKey).</li> <li>Define how you handle ambiguous matches (keep all candidates vs select one; how ties are resolved).</li> </ul> </li> <li> <p>Pick an analysis method aligned with the input</p> <ul> <li>ORA/MSEA on a significant list: simple, but sensitive to thresholds and overlap.</li> <li>Rank-based approaches: less dependent on arbitrary cutoffs.</li> <li>Topology/network-aware scoring: can improve prioritization but does not eliminate overlap/ambiguity.</li> </ul> </li> <li> <p>Use an assay-appropriate universe/background</p> <ul> <li>Avoid \u201call metabolites in the database\u201d.</li> <li>Use metabolites measurable/detectable in your assay (or the QC-passing set).</li> </ul> </li> <li> <p>Summarize results as themes and show the drivers</p> <ul> <li>Group redundant pathways into themes.</li> <li>For each theme/pathway, list the contributing metabolites/features, their direction of change, and confidence.</li> </ul> </li> </ol> <p>Reference: Metabolites are not genes \u2014 avoiding the misuse of pathway analysis in metabolomics (2025).</p>"},{"location":"workflow/stat_intro/","title":"Statistical analysis: introduction","text":"<p>Statistical analysis is a crucial step in the metabolomics workflow, as it allows researchers to extract meaningful insights from complex datasets. The choice of statistical methods depends on the research question, the experimental design, and the nature of the data.</p> <p>This workflow splits statistical analysis into:</p> <ul> <li>Univariate analysis</li> <li>Multivariate analysis</li> <li>Pathway analysis</li> </ul>"},{"location":"workflow/stat_intro/#data-preparation","title":"Data preparation","text":"<p>Before performing statistical analysis, it is essential to check and prepare the data. Here we assume the data has been preprocessed, missing values have been imputed, and the data has been normalized/batch corrected.</p>"},{"location":"workflow/stat_intro/#outlier-removal","title":"Outlier removal","text":"<p>Outliers can significantly affect statistical results. Identify and handle outliers using a combination of visualization and statistical/multivariate diagnostics.</p> <ul> <li>Visual inspection<ul> <li>IQR method: values outside 1.5\u00d7IQR from Q1/Q3 are considered outliers.    considered outliers.</li> </ul> </li> <li>Multivariate methods<ul> <li>PCA: samples far from the main cluster in score plots may be outliers.</li> <li>Mahalanobis distance: samples with large distance from the center may be flagged.</li> <li>Hotelling\u2019s T-squared: samples exceeding a threshold may be flagged.</li> <li>Q residuals: samples with high Q residuals in PCA may indicate outliers.</li> </ul> </li> </ul> <p>A Hotelling\u2019s T-squared vs Q residuals plot (e.g., using PC1\u2013PC3) can help identify outliers in multivariate space.</p>"},{"location":"workflow/stat_intro/#data-transformation","title":"Data transformation","text":"<p>Transformation is typically applied after normalization/batch correction and before scaling to make feature distributions more suitable for statistical modeling. Common goals are to reduce right-skewness, mitigate heteroscedasticity (mean\u2013variance dependence), reduce the influence of extreme values, and make relationships more approximately linear/additive.</p> <p>Common transformations:</p> <ul> <li>Log transform (often <code>log2</code> or <code>log10</code>): widely used because intensities are frequently right-skewed and effects are often multiplicative.<ul> <li>Practical note: handle zeros/small values using an offset, e.g. <code>log(x + \u03b5)</code>, or use <code>log1p(x)</code> where appropriate. Report the chosen approach.</li> </ul> </li> <li>Square-root transform: milder than log; useful when log is too aggressive.</li> <li>Box\u2013Cox (positive data) or Yeo\u2013Johnson (allows zeros/negatives): data-driven families that can stabilize variance.</li> </ul>"},{"location":"workflow/stat_intro/#data-scaling","title":"Data scaling","text":"<p>After transformation, scaling is applied to make features more comparable, so that models are not dominated by high-variance or high-abundance metabolites/lipids. Scaling is especially important for variance-sensitive multivariate methods (e.g., PCA, PLS(-DA), clustering).</p> <p>Common choices:</p> <ul> <li>Mean-centering: subtract the feature mean (centers values around 0; does not change variance).</li> <li>Auto-scaling (unit-variance scaling; z-scoring): mean-center and divide by SD (mean = 0, SD = 1); can amplify noise for low-precision features.</li> <li>Pareto scaling: mean-center and divide by sqrt(SD); compromise between no scaling and auto-scaling.</li> </ul> <p>Choose scaling based on the analysis goal and report the method used.</p>"},{"location":"workflow/tar_acceptance/","title":"Acceptance Criteria","text":"<p>Clear, objective acceptance criteria are essential for ensuring the reliability, reproducibility, and scientific integrity of targeted metabolomics and lipidomics data. These criteria should be defined before data acquisition and consistently applied during data processing and review.</p>"},{"location":"workflow/tar_acceptance/#calibration-curve-acceptance","title":"Calibration Curve Acceptance","text":"<p>Calibration curve acceptance is critical for ensuring that quantitative results are accurate and reliable. The calibration curve describes how instrument response relates to analyte concentration, and its quality directly affects all downstream quantification.</p> <ul> <li>Curve fit quality:<ul> <li>The most common metric for curve fit is the coefficient of determination (R\u00b2). For linear models, a typical acceptance threshold is R\u00b2 \u2265 0.99, indicating that the model explains at least 99% of the variance in the data.</li> <li>However, R\u00b2 alone is not sufficient. Back-calculated accuracy for each calibrator should also be assessed. This means calculating the concentration of each calibrator using the fitted curve and comparing it to the known value. Acceptance is usually within \u00b115% of the nominal value for most points, and \u00b120% at the lower limit of quantification (LLOQ).</li> </ul> </li> <li>Residuals and outliers:<ul> <li>Residuals are the differences between observed and predicted responses. Plotting residuals helps identify systematic deviations (e.g., curvature, heteroscedasticity) that may indicate a poor model fit or the need for a different calibration model or weighting.</li> <li>Outlier calibrators should only be excluded if there is a clear justification (e.g., pipetting error, instrument malfunction). All exclusions must be documented, and the impact on curve fit and accuracy should be reassessed after exclusion.</li> </ul> </li> <li>LLOQ/ULOQ:<ul> <li>The lower limit of quantification (LLOQ) is the lowest concentration at which the analyte can be reliably quantified with acceptable accuracy and precision. The upper limit (ULOQ) is the highest such concentration.</li> <li>LLOQ and ULOQ should be established during method validation and confirmed in each batch. Only report concentrations within this validated range; values outside should be flagged as below or above quantification limits.</li> </ul> </li> <li>Model and weighting selection:<ul> <li>Choose the simplest model that provides acceptable fit and accuracy (linear is preferred unless non-linearity is evident).</li> <li>Weighting (e.g., 1/x, 1/x\u00b2) can improve fit at low concentrations by reducing the influence of high-concentration points. The choice of weighting should be justified by improved accuracy and residual distribution.</li> </ul> </li> </ul>"},{"location":"workflow/tar_acceptance/#qc-precision-and-accuracy","title":"QC Precision and Accuracy","text":"<p>Quality control (QC) samples are used to monitor the precision and accuracy of the analytical process. They provide confidence that the method is performing as expected throughout the batch.</p> <ul> <li>Pooled QC samples:<ul> <li>Pooled QCs are typically prepared by combining small aliquots from multiple study samples. They represent the average sample matrix and are injected repeatedly throughout the batch.</li> <li>Precision is assessed by calculating the coefficient of variation (%CV) for each analyte across all pooled QC injections. A common acceptance threshold is %CV \u2264 15%, indicating good reproducibility. Higher %CV may indicate instrument drift, sample instability, or other technical issues.</li> </ul> </li> <li>Calibration QC samples:<ul> <li>Calibration QCs are samples with known concentrations, distinct from the calibrators used to build the curve. They are used to independently verify the accuracy of quantification.</li> <li>Measured concentrations should be within \u00b115% of the nominal value for most QCs. Consistent failure to meet this criterion suggests problems with calibration, sample preparation, or instrument performance.</li> </ul> </li> <li>Blanks:<ul> <li>Blanks (solvent or extraction blanks) are used to check for contamination or carryover. Analyte signals in blanks should be less than 20% of the LLOQ response. Higher signals may indicate contamination, carryover, or background noise, and should prompt investigation.</li> </ul> </li> <li>Ongoing monitoring:<ul> <li>Plot QC and blank results over the course of the batch to detect trends, shifts, or outliers. Investigate and document any deviations from expected performance.</li> </ul> </li> </ul> <p>By understanding and applying these acceptance criteria, you ensure that only high-quality, reliable data are reported and interpreted, and that any issues are detected and addressed promptly.</p>"},{"location":"workflow/tar_acquisition/","title":"Acquisition Methods in Targeted Metabolomics/Lipidomics","text":"<p>Selecting the appropriate data acquisition method is crucial for achieving the goals of a targeted metabolomics or lipidomics study. Different acquisition strategies offer distinct advantages depending on the number of targets, required sensitivity, selectivity, and data analysis needs.</p>"},{"location":"workflow/tar_acquisition/#multiple-reaction-monitoring-mrmsrm","title":"Multiple Reaction Monitoring (MRM/SRM)","text":"<ul> <li>Description:<ul> <li>MRM (also called SRM) is a targeted MS/MS approach where specific precursor/product ion pairs (transitions) are monitored for each analyte.</li> </ul> </li> <li>Best suited for:<ul> <li>Large panels of analytes where high sensitivity, selectivity, and quantitative robustness are required.</li> <li>High-throughput studies, clinical assays, and regulatory applications.</li> </ul> </li> <li>Advantages:<ul> <li>Excellent sensitivity and reproducibility.</li> <li>High selectivity for target compounds.</li> <li>Well-established for quantitative workflows.</li> </ul> </li> <li>Limitations:<ul> <li>Limited flexibility for post-acquisition data analysis.</li> <li>Requires prior knowledge of all targets and transitions.</li> </ul> </li> </ul>"},{"location":"workflow/tar_acquisition/#parallel-reaction-monitoring-prm","title":"Parallel Reaction Monitoring (PRM)","text":"<ul> <li>Description:<ul> <li>PRM is a high-resolution targeted MS/MS method where all fragment ions of a selected precursor are acquired simultaneously.</li> </ul> </li> <li>Best suited for:<ul> <li>Smaller to medium-sized panels where high selectivity and mass accuracy are needed.</li> <li>Situations where post-acquisition selection of fragment ions or confirmation of analyte identity is important.</li> </ul> </li> <li>Advantages:<ul> <li>High selectivity and mass accuracy.</li> <li>Flexibility to select or confirm fragments after acquisition.</li> </ul> </li> <li>Limitations:<ul> <li>Lower throughput compared to MRM for very large panels.</li> </ul> </li> </ul>"},{"location":"workflow/tar_acquisition/#data-independent-acquisition-dia","title":"Data-Independent Acquisition (DIA)","text":"<ul> <li>Description:<ul> <li>DIA methods (e.g., SWATH, MS^E) acquire MS/MS data for all ions within defined m/z windows, enabling comprehensive coverage.</li> </ul> </li> <li>Best suited for:<ul> <li>Studies requiring broad coverage and retrospective data mining.</li> <li>Targeted quantification when combined with appropriate data processing workflows.</li> </ul> </li> <li>Advantages:<ul> <li>Enables both targeted and untargeted analysis from the same dataset.</li> <li>Allows for post-acquisition discovery of new targets.</li> </ul> </li> <li>Limitations:<ul> <li>More complex data analysis and potential for increased interference.</li> </ul> </li> </ul>"},{"location":"workflow/tar_acquisition/#full-scan-targeted-extraction","title":"Full Scan (Targeted Extraction)","text":"<ul> <li>Description:<ul> <li>Full scan acquisition collects MS1 spectra across a wide m/z range, with targeted quantification performed by extracting signals for specific m/z values.</li> </ul> </li> <li>Best suited for:<ul> <li>Small panels or when only precursor ion information is needed.</li> <li>Situations where maximum flexibility for post-acquisition analysis is desired.</li> </ul> </li> <li>Advantages:<ul> <li>Simple setup and broad data collection.</li> <li>Allows for retrospective analysis of additional targets.</li> </ul> </li> <li>Limitations:<ul> <li>Lower sensitivity and selectivity compared to MRM/PRM.</li> </ul> </li> </ul>"},{"location":"workflow/tar_acquisition/#choosing-an-acquisition-method","title":"Choosing an Acquisition Method","text":"<ul> <li>Consider the number of targets, required sensitivity/selectivity, sample throughput, and data analysis needs.</li> <li>MRM is preferred for large, quantitative panels; PRM for high selectivity and confirmation; DIA for broad coverage and discovery; full scan for flexibility and small panels.</li> <li>The choice of method should align with study goals and available instrumentation.</li> </ul>"},{"location":"workflow/tar_batch_qc/","title":"Batch Design and Quality Control (QC)","text":"<p>Batch design and QC principles in targeted metabolomics/lipidomics are largely similar to those in untargeted workflows. For general guidance on sample randomization, pooled QCs, blanks, and batch documentation, see untargeted QC guidelines.</p>"},{"location":"workflow/tar_batch_qc/#key-differences-in-targeted-qc","title":"Key Differences in Targeted QC","text":"<ul> <li>Calibration standards and calibration QC samples:<ul> <li>Targeted workflows require the inclusion of calibration standards and calibration QC samples in each batch to enable absolute quantification and monitor calibration curve stability.</li> <li>Calibration samples should span the expected concentration range and be injected at appropriate intervals (e.g., beginning, end, and/or throughout the batch).</li> </ul> </li> <li>Internal standard (IS) mapping:<ul> <li>Targeted workflows often use a defined mapping of internal standards to specific analytes or classes, and normalization is performed accordingly during data processing.</li> </ul> </li> <li>Documentation of calibration and IS performance:<ul> <li>More rigorous tracking and reporting of calibration curve parameters, IS response, and quantification accuracy are expected in targeted workflows.</li> </ul> </li> </ul> <p>For all other aspects of batch design and QC, refer to the untargeted QC documentation.</p>"},{"location":"workflow/tar_calibration/","title":"Calibration","text":"<p>Calibration is the process of establishing the quantitative relationship between instrument response and analyte concentration using a series of calibration standards. This step is essential for converting raw signal intensities into absolute concentrations in targeted metabolomics and lipidomics.</p>"},{"location":"workflow/tar_calibration/#calibration-in-data-processing","title":"Calibration in Data Processing","text":"<ul> <li>Purpose:<ul> <li>Calibration curves enable the transformation of instrument response (peak area or height) into meaningful concentration values for each analyte.</li> <li>Each batch should include a full set of calibration standards to ensure accuracy and reproducibility.</li> </ul> </li> <li>Data workflow:<ul> <li>Acquire calibration standards, study samples, and QCs together in each batch.</li> <li>Extract peak areas/heights for each analyte and internal standard in all calibration samples.</li> <li>Calculate response ratios (analyte/IS) if internal standards are used.</li> <li>Plot response (or response ratio) versus known concentration for each analyte.</li> <li>Fit an appropriate regression model (e.g., linear, quadratic, weighted) to the calibration data.</li> <li>Assess curve quality using R\u00b2, residuals, back-calculated accuracy, and visual inspection.</li> <li>Use the calibration model to interpolate concentrations for unknown samples.</li> </ul> </li> <li>Documentation:<ul> <li>Store calibration curve parameters, sample metadata, and QC results for traceability.</li> </ul> </li> </ul>"},{"location":"workflow/tar_calibration/#key-concepts","title":"Key Concepts","text":"<ul> <li>Calibrator / standard level: Sample with a known analyte concentration.</li> <li>Response: Typically peak area or height, often normalized as an area ratio (analyte / internal standard).</li> <li>Calibration model: Regression equation linking response to concentration.</li> <li>LLOQ / ULOQ: Lower/upper limits of quantification (bounds of reliable quantitation).</li> <li>LOD: Limit of detection (lowest detectable, not always quantifiable).</li> </ul>"},{"location":"workflow/tar_calibration/#calibration-standards-and-internal-standards","title":"Calibration Standards and Internal Standards","text":"<ul> <li>External standards (calibration levels):<ul> <li>Prepare a multi-level calibration series spanning the expected biological concentration range.</li> <li>Include a blank (no analyte) and a zero (internal standard only) if appropriate.</li> <li>Record exact concentrations and sample IDs for each calibrator.</li> </ul> </li> <li>Internal standards (IS):<ul> <li>Use stable isotope-labeled IS for key analytes when possible.</li> <li>For other analytes, use class- or chemistry-matched IS and document the mapping.</li> <li>Add IS to all samples, calibrators, and QCs at a constant concentration, ideally before extraction.</li> <li>Normalize analyte response to IS in data processing.</li> </ul> </li> </ul>"},{"location":"workflow/tar_calibration/#calibration-curve-design","title":"Calibration Curve Design","text":"<ul> <li>Concentration range and number of levels:<ul> <li>Use enough calibration levels (typically 6\u201310+) to characterize the curve across the intended range.</li> <li>Ensure the range covers anticipated sample concentrations to minimize extrapolation.</li> </ul> </li> <li>Curve fitting and evaluation:<ul> <li>Fit the most appropriate model for each analyte (linear, quadratic, weighted, etc.).</li> <li>Evaluate fit quality using R\u00b2, residual plots, and back-calculated concentrations.</li> <li>Define and document acceptance criteria for curve fit and quantification limits.</li> </ul> </li> </ul>"},{"location":"workflow/tar_intro/","title":"Targeted metabolomics/lipidomics","text":"<p>Targeted metabolomics/lipidomics measures a pre-defined list of analytes using optimized acquisition and quantification. Compared to untargeted workflows, targeted studies typically provide higher confidence identification and more robust quantitation (often absolute concentration).</p>"},{"location":"workflow/tar_intro/#overview","title":"Overview","text":"<p>This section introduces the targeted data processing workflow, highlighting the main steps and key differences from untargeted analysis. Each step will be described in detail in dedicated pages.</p>"},{"location":"workflow/tar_intro/#key-differences-from-untargeted-workflow","title":"Key differences from untargeted workflow","text":"<ul> <li>Calibration curves for absolute quantification</li> <li>Use of internal and external standards</li> <li>Predefined analyte list and optimized acquisition</li> <li>Stringent QC and acceptance criteria</li> </ul>"},{"location":"workflow/tar_intro/#workflow-steps","title":"Workflow steps","text":"<ul> <li>Targets and assay design: Define the list of analytes to be measured and design the assay to ensure optimal detection and quantification for each target compound.</li> <li>Standards and internal standards: Select and prepare calibration standards and internal standards to enable accurate quantification and correction for technical variability.</li> <li>Calibration: Generate calibration curves using known concentrations of standards to establish the relationship between instrument response and analyte concentration.</li> <li>Batch design and QC: Plan the sample acquisition order and incorporate quality control (QC) samples, blanks, and calibrators to monitor analytical performance and data quality throughout the batch.</li> <li>Acquisition (MRM/PRM): Acquire data using optimized multiple reaction monitoring (MRM) or parallel reaction monitoring (PRM) methods for sensitive and selective detection of target analytes.</li> <li>Data processing: Integrate peaks, normalize using internal standards, apply calibration curves, and calculate absolute concentrations. Perform quality checks at each step.</li> <li>Acceptance criteria: Define and apply objective criteria for calibration curve acceptance, QC precision, and sample inclusion/exclusion to ensure data reliability.</li> </ul>"},{"location":"workflow/tar_processing/","title":"Data Processing","text":"<p>Data processing in targeted metabolomics/lipidomics transforms raw instrument data into quantitative results ready for interpretation. Each step should be performed systematically and documented for reproducibility.</p>"},{"location":"workflow/tar_processing/#typical-data-processing-workflow","title":"Typical Data Processing Workflow","text":"<ul> <li>Peak integration:<ul> <li>Use validated software to detect and integrate peaks for each target analyte and internal standard.</li> <li>Review and, if necessary, manually adjust integrations to ensure accuracy, especially for low-abundance or poorly resolved peaks.</li> <li>Export integrated peak areas or heights for all samples, calibrators, QCs, and blanks.</li> </ul> </li> <li>Normalization:<ul> <li>Normalize analyte responses to the corresponding internal standard (e.g., analyte/IS ratio) to correct for technical variability.</li> <li>For analytes without a direct IS, use class- or chemistry-matched normalization as documented in the IS mapping.</li> </ul> </li> <li>Calibration and quantification:<ul> <li>Apply calibration curves to normalized responses to calculate absolute concentrations for each analyte.</li> <li>Calibration models:<ul> <li>Common models include linear, quadratic, and occasionally cubic regression.</li> <li>The choice of model depends on the analyte\u2019s response range and curve shape; linear is most common, but higher-order models may be needed for non-linear responses.</li> </ul> </li> <li>Weighting strategies:<ul> <li>Weighting (e.g., 1/x, 1/x\u00b2) is often applied to reduce the influence of high-concentration calibrators and improve fit at lower concentrations.</li> <li>The optimal weighting is determined by evaluating residuals and back-calculated accuracy across the calibration range.</li> </ul> </li> <li>Interpolate unknown sample concentrations from the calibration model; flag any values outside the validated range (LLOQ/ULOQ).</li> </ul> </li> <li>Quality control and review:<ul> <li>Assess QC sample results (e.g., %CV across pooled QCs, accuracy of calibration QCs) to verify data quality.</li> <li>Flag and investigate outliers, integration errors, or QC failures.</li> <li>Exclude or annotate problematic data points as needed, documenting all decisions.</li> </ul> </li> </ul>"},{"location":"workflow/tar_standards/","title":"Standards and internal standards","text":"<p>Accurate quantification in targeted metabolomics and lipidomics relies on the careful use of standards\u2014both calibration standards and internal standards (IS). These are essential for ensuring that your results are reliable, comparable, and scientifically meaningful.</p>"},{"location":"workflow/tar_standards/#general-use-and-rationale","title":"General use and rationale","text":"<p>Standards are used throughout the analytical workflow to:</p> <ul> <li>Enable absolute quantification of analytes (calibration standards)</li> <li>Correct for technical variability and improve precision (internal standards)</li> <li>Monitor instrument performance and data quality</li> <li>Support troubleshooting and reproducibility</li> </ul> <p>In most workflows, standards are included in every batch and are processed alongside study samples and QCs. Their correct use is a hallmark of robust experimental design.</p>"},{"location":"workflow/tar_standards/#calibration-standards","title":"Calibration standards","text":"<p>Calibration standards are samples with known concentrations of target analytes. They are used to generate a calibration curve, which converts instrument response (e.g., peak area) into absolute concentration.</p> <p>Key steps:</p> <ul> <li> <p>Preparation:</p> <ul> <li>Prepare a series of calibration samples covering the expected concentration range.</li> <li>Record concentrations and sample IDs for each calibrator.</li> <li>Tip: Use serial dilutions for accuracy and reproducibility.</li> </ul> </li> <li> <p>Data acquisition:</p> <ul> <li>Run calibration standards in each batch, alongside study samples and QCs.</li> <li>Randomize order to minimize batch effects.</li> </ul> </li> <li> <p>Data processing:</p> <ul> <li>Extract peak areas for each analyte in each calibration sample.</li> <li>Plot instrument response versus known concentration to build the calibration curve.</li> <li>Fit an appropriate model (e.g., linear, quadratic, weighted regression). Assess curve quality (R\u00b2, residuals, back-calculated accuracy).</li> <li>Use the calibration curve to interpolate concentrations in unknown samples.</li> </ul> </li> <li> <p>Documentation:</p> <ul> <li>Save calibration curve parameters and sample metadata for traceability and reproducibility.</li> </ul> </li> </ul> <p>Practical tip: Always check the linearity and accuracy of your calibration curve before quantifying study samples. Poor calibration undermines all downstream results.</p>"},{"location":"workflow/tar_standards/#internal-standards-is","title":"Internal standards (IS)","text":"<p>Internal standards are compounds added at a constant amount to all samples, calibrators, and QCs. They are chemically similar to the analytes of interest but distinguishable by the instrument (e.g., isotopically labeled analogs).</p> <p>Key steps:</p> <ul> <li>Addition:<ul> <li>Add IS to all samples, calibrators, and QCs before extraction.</li> <li>Record IS identity, concentration, and addition protocol.</li> <li>Choose IS that closely match the chemical properties of your analytes.</li> </ul> </li> <li>Data acquisition:<ul> <li>Ensure IS signals are acquired in every run, are well-resolved, and free from interference.</li> </ul> </li> <li>Data processing:<ul> <li>Normalize analyte peak areas or heights to the corresponding IS (e.g., analyte/IS ratio).</li> <li>Use normalized values for calibration curve construction and quantification.</li> <li>Monitor IS response across all samples to detect technical issues.</li> </ul> </li> <li>Quality control:<ul> <li>Flag samples with abnormal IS response for review or exclusion.</li> <li>Plot IS response across the batch to visualize trends or outliers.</li> </ul> </li> </ul> <p>Practical tip: Consistent IS response across samples is a strong indicator of technical stability. Sudden changes may signal problems that require investigation.</p>"},{"location":"workflow/tar_targets/","title":"Targets and assay design","text":"<p>Careful selection and documentation of target analytes is the foundation of a robust targeted metabolomics or lipidomics workflow. The choices made at this stage directly impact data quality, interpretability, and reproducibility.</p>"},{"location":"workflow/tar_targets/#defining-the-target-list","title":"Defining the target list","text":"<ul> <li> <p>Selection criteria:</p> <ul> <li>Choose analytes based on biological relevance, study goals, and analytical feasibility.</li> <li>Consider pathway coverage, disease association, or hypothesis-driven targets.</li> </ul> </li> <li> <p>Documentation:</p> <ul> <li>For each analyte, record:<ul> <li>Name and stable identifier (e.g., InChIKey, HMDB, KEGG, ChEBI)</li> <li>Expected ion form(s) and polarity (e.g., [M+H]+, [M-H]-)</li> <li>Expected retention time window or RT index</li> <li>Expected fragments/transitions (for MRM/PRM)</li> <li>Reference spectra or links to spectral libraries (if available)</li> </ul> </li> <li>Maintain a structured target list (spreadsheet or database) for traceability and downstream processing.</li> </ul> </li> </ul>"},{"location":"workflow/tar_targets/#acquisition-strategy","title":"Acquisition strategy","text":"<ul> <li> <p>MRM/SRM (triple quadrupole):</p> <ul> <li>Suitable for large panels requiring high sensitivity and robust quantitation.</li> <li>Enables scheduled acquisition for high throughput.</li> </ul> </li> <li> <p>PRM / high-resolution targeted:</p> <ul> <li>Useful for smaller panels or when high selectivity and mass accuracy are needed.</li> <li>Allows retrospective data analysis and flexible fragment selection.</li> </ul> </li> <li> <p>Strategy selection:</p> <ul> <li>Match acquisition strategy to study needs, instrument capabilities, and data analysis requirements.</li> </ul> </li> </ul>"},{"location":"workflow/tar_targets/#method-design-constraints","title":"Method design constraints","text":"<ul> <li> <p>Concentration range and LLOQ:</p> <ul> <li>Estimate expected analyte concentrations and ensure the method covers this range with acceptable accuracy and precision.</li> <li>Define lower and upper limits of quantification (LLOQ/ULOQ) for each analyte.</li> </ul> </li> <li> <p>Sample matrix and extraction:</p> <ul> <li>Consider matrix effects and select extraction methods compatible with target analytes.</li> <li>Plan for matrix-matched calibration if needed.</li> </ul> </li> <li> <p>Sample throughput and batch size:</p> <ul> <li>Determine the number of samples, QCs, and calibrators per batch for robust data and manageable instrument time.</li> </ul> </li> </ul>"},{"location":"workflow/univariate/","title":"Univariate Analysis","text":"<p>Univariate analysis examines each variable (feature, metabolite, or lipid) independently to identify significant differences between groups or associations with outcomes. This approach is foundational in metabolomics and lipidomics for hypothesis testing and biomarker discovery.</p>"},{"location":"workflow/univariate/#what-is-univariate-analysis","title":"What is Univariate Analysis?","text":"<ul> <li>Focuses on one variable at a time, ignoring correlations with other variables.</li> <li>Commonly used to:<ul> <li>Compare groups (e.g., disease vs. control)</li> <li>Assess associations with continuous outcomes (e.g., age, BMI)</li> <li>Screen for potential biomarkers</li> </ul> </li> </ul>"},{"location":"workflow/univariate/#group-comparison-tests","title":"Group Comparison Tests","text":"<ul> <li>t-tests:<ul> <li>Compare the means of two groups (e.g., control vs. treatment).</li> <li>Assumes data are normally distributed and have equal variances (use Welch\u2019s t-test if variances differ).</li> </ul> </li> <li>ANOVA (Analysis of Variance):<ul> <li>Compare means across three or more groups.</li> <li>Assumes normality and equal variances; significant results indicate at least one group differs.</li> </ul> </li> <li>Mann\u2013Whitney U test:<ul> <li>Non-parametric alternative to the t-test for two groups.</li> <li>Does not assume normality; compares medians or ranks.</li> </ul> </li> <li>Kruskal\u2013Wallis test:<ul> <li>Non-parametric alternative to ANOVA for three or more groups.</li> <li>Tests for differences in distributions across groups.</li> </ul> </li> </ul>"},{"location":"workflow/univariate/#post-hoc-testing","title":"Post-hoc Testing","text":"<ul> <li>After a significant ANOVA or Kruskal\u2013Wallis result, use post-hoc tests (e.g., Tukey\u2019s HSD, Dunn\u2019s test) to determine which specific groups differ.</li> <li>Post-hoc tests control for multiple comparisons and help interpret complex group differences.</li> </ul>"},{"location":"workflow/univariate/#multiple-testing-correction","title":"Multiple Testing Correction","text":"<ul> <li>When testing many features, the chance of false positives increases.</li> <li>Apply corrections such as:<ul> <li>Bonferroni correction: Very stringent; divides the significance threshold by the number of tests.</li> <li>Benjamini\u2013Hochberg FDR: Controls the expected proportion of false discoveries; less stringent and widely used in omics.</li> </ul> </li> <li>Always report both raw and adjusted p-values.</li> </ul>"},{"location":"workflow/univariate/#correlation-analysis","title":"Correlation Analysis","text":"<ul> <li>Correlation analysis:<ul> <li>Measures the strength and direction of association between a continuous outcome (e.g., age, clinical measurement) and each  feature.</li> <li>Pearson correlation: Assumes linear relationship and normality.</li> <li>Spearman correlation: Non-parametric; assesses monotonic relationships using ranks.</li> </ul> </li> <li>Partial correlation analysis:</li> <li>Measures the association between two variables while controlling for the effect of one or more additional variables (e.g., adjusting for age or sex).</li> </ul>"},{"location":"workflow/untar_clean/","title":"Peak cleaning","text":"<p>Untargeted LC\u2013MS(/MS) feature tables commonly contain non-biological signals: solvent/reagent impurities, labware contaminants (e.g., plasticizers), column bleed, in-source fragments, and carryover from prior injections. Peak cleaning aims to remove features dominated by these artifacts and by unstable integration, so downstream statistics are driven by reproducible biology rather than technical noise.</p> <p>This step is typically performed after peak detection/alignment and before statistical analysis. Thresholds depend on platform, matrix, and study goals; treat the values below as starting points and document what you used.</p>"},{"location":"workflow/untar_clean/#core-cleaning-rules","title":"Core cleaning rules","text":"<p>Apply filters in a consistent order (e.g., blanks \u2192 QC precision \u2192 missingness \u2192 intensity/RT sanity checks). Keep a record of how many features are removed at each step.</p>"},{"location":"workflow/untar_clean/#1-blank-based-filtering-contamination-and-carryover","title":"1) Blank-based filtering (contamination and carryover)","text":"<p>Goal: remove features that are largely explained by blank signal.</p> <p>Common rules (choose one and keep it consistent):</p> <ul> <li>Sample/blank ratio: remove if the mean (or median) intensity in study samples is not sufficiently higher than blanks.<ul> <li>Example rule: remove if $\\text{mean(sample)} \\le 3 \\times \\text{mean(blank)}$.</li> </ul> </li> <li>Blank prevalence: remove if the feature is present in blanks above a minimum intensity in a large fraction of blank injections.</li> </ul> <p>Notes:</p> <ul> <li>Use process blanks to catch extraction-derived artifacts; use solvent blanks to monitor instrument carryover.</li> <li>If you observe carryover (features appearing right after high-intensity injections), consider tightening wash conditions in acquisition; filtering alone may not fully fix it.</li> </ul>"},{"location":"workflow/untar_clean/#2-qc-precision-filtering-pooled-qc-based","title":"2) QC precision filtering (pooled-QC based)","text":"<p>Goal: remove features that are not measured reproducibly.</p> <p>Compute variability in pooled QCs and filter features with poor precision.</p> <ul> <li>QC RSD%: it is the relative standard deviation (coefficient of variation) of feature intensities in pooled QCs. A common starting threshold is removing features with RSD% &gt; 30%.</li> <li>D-ratio: it is the ratio of technical variation (e.g., RSD% in QCs) to biological variation (e.g., RSD% in samples). A common rule is to remove features with D-ratio &gt; 0.8, which indicates technical variation dominates biological variation.</li> </ul>"},{"location":"workflow/untar_clean/#3-dilution-series-based-filtering","title":"3) Dilution series-based filtering","text":"<p>Goal: remove features that do not show a consistent response to dilution, which indicates they are likely noise or artifacts.</p> <ul> <li>If you have a QC dilution series (e.g., serial dilutions of a pooled QC), compute the correlation between feature intensity and dilution factor across the series.</li> <li>Remove features that do not show a strong positive correlation (e.g., Pearson\u2019s r &lt; 0.7) with dilution, as they are unlikely to represent true biological signals.</li> </ul>"},{"location":"workflow/untar_clean/#4-missingness-filtering","title":"4) Missingness filtering","text":"<p>Goal: remove features with too many missing values to support robust inference.</p> <p>Common rules:</p> <ul> <li>Remove features with high missingness in pooled QCs (often indicates unstable detection/integration).</li> <li>Remove features with high missingness in study samples (threshold depends on design; for example, require presence in at least a set fraction of samples within a group).</li> </ul>"},{"location":"workflow/untar_clean/#5-intensity-floor-signal-to-noise-sanity-check","title":"5) Intensity floor (signal-to-noise sanity check)","text":"<p>Goal: remove very low-intensity features that are close to the detection limit and tend to be unstable.</p> <ul> <li>Remove features whose mean/median intensity in pooled QCs is below a minimum.<ul> <li>Example: a fixed threshold (e.g., 500 arbitrary units) can be used as a dataset-specific floor, but should be justified from your instrument/noise level.</li> </ul> </li> </ul> <p>Prefer defining the floor based on observed noise (or an instrument-specific criterion) rather than copying a universal number.</p>"},{"location":"workflow/untar_clean/#6-retention-time-sanity-checks","title":"6) Retention time sanity checks","text":"<p>Goal: remove features in regions known to be unreliable.</p> <ul> <li>Remove very early eluting features if they are dominated by injection/solvent front effects in your method.</li> <li>Optionally remove features outside the chromatographic window where your gradient contains meaningful separation.</li> </ul> <p>Define \u201cearly eluting\u201d using an RT cut (minutes) that matches your LC method.</p>"},{"location":"workflow/untar_identification/","title":"Identification","text":"<p>In MS-based metabolomics/lipidomics, identification is to accumulate evidence that a measured feature corresponds to a chemical structure (or at least a chemical class). Because multiple compounds can share similar $m/z$, fragments, and chromatographic behavior, identification is typically probabilistic and should be reported with a clearly defined confidence level.</p>"},{"location":"workflow/untar_identification/#identification-levels","title":"Identification levels","text":"<p>Before we dive into specific approaches, it\u2019s important to establish a common framework for communicating identification confidence. The widely cited Schymanski et al. 5-level scheme (2014) provides a useful structure for this purpose:</p> <p></p> <p>This 5-level scheme communicates how strongly the experimental evidence supports an assignment. Levels 1\u20132 aim at one exact structure (with different types of support), while Levels 3\u20135 reflect decreasing structural specificity (from candidate set, to formula, to mass of interest).</p>"},{"location":"workflow/untar_identification/#level-1-confirmed-structure","title":"Level 1 \u2014 Confirmed structure","text":"<p>Ideal case: the proposed structure is confirmed with an authentic reference standard measured with the same (or sufficiently comparable) method.</p> <ul> <li>Requires MS, MS/MS, and retention time matching to the reference standard.</li> <li>Recommended: add an orthogonal method when possible (e.g., different separation mode, ion mobility/CCS, NMR, etc.) to further reduce the risk of false confirmation.</li> </ul>"},{"location":"workflow/untar_identification/#level-2-probable-structure-exact-structure-proposed-without-an-in-house-standard","title":"Level 2 \u2014 Probable structure (exact structure proposed without an in-house standard)","text":"<p>Evidence strongly supports one exact structure, but confirmation with a measured standard is missing. Two common routes:</p> <p>Level 2a \u2014 Library (spectrum/library match)</p> <ul> <li>Identification is based on an unambiguous spectrum\u2013structure match to literature or spectral libraries.</li> <li>Caution: comparisons across different acquisition conditions can be misleading. Parameters such as instrument type/resolution, collision energy, ionization mode, and MS level should be considered, and decision criteria (scores/thresholds) should be reported.</li> <li>Desirable add-on evidence: retention behavior. This is well established via retention indices in GC\u2013MS, but is less standardized for LC\u2013MS (so RT evidence is often less portable across labs).</li> </ul> <p>Level 2b \u2014 Diagnostic (diagnostic evidence, no library/standard)</p> <ul> <li>Used when no alternative structure fits the experimental evidence, even though no standard or library match exists.</li> <li>Evidence can include diagnostic MS/MS fragments/neutral losses, characteristic ionization/adduct behavior, information about the parent compound, and the experimental context (e.g., expected transformation pathways).</li> </ul> <p>For practical reporting, it is often sufficient to report \u201cLevel 2: probable structure\u201d, even if you internally distinguish 2a vs 2b.</p>"},{"location":"workflow/untar_identification/#level-3-tentative-candidates-multiple-plausible-structures","title":"Level 3 \u2014 Tentative candidate(s) (multiple plausible structures)","text":"<p>A \u201cgrey zone\u201d: evidence supports possible structure(s), but is insufficient to select one exact structure (common with positional isomers or closely related analogs).</p> <ul> <li>Typical sources: accurate mass + partial MS/MS consistency; database candidates ranked similarly; in silico fragmentation suggesting several top candidates.</li> <li>Because situations vary widely, Schymanski et al. recommend avoiding overly generic sublevels; if needed, define study-specific sublevels and present the evidence for each candidate, especially when the identity is central to conclusions.</li> </ul>"},{"location":"workflow/untar_identification/#level-4-unequivocal-molecular-formula","title":"Level 4 \u2014 Unequivocal molecular formula","text":"<p>A molecular formula can be assigned unambiguously, but there is not enough evidence to propose a structure.</p> <ul> <li>Formula assignment may use adduct patterns, isotope patterns, and/or fragments.</li> <li>MS/MS may be absent, uninformative, or affected by interferences.</li> <li>Still valuable to report: a reliable formula can be searched, traced, and re-identified in future studies.</li> </ul>"},{"location":"workflow/untar_identification/#level-5-exact-mass-mz-of-interest-feature-only","title":"Level 5 \u2014 Exact mass (m/z) of interest (feature only)","text":"<p>A feature is defined by exact mass (m/z) (often with RT), but there is insufficient information to assign even a formula.</p> <ul> <li>Can still record and store MS/MS as an \u201cunknown spectrum\u201d for future matching.</li> <li>Should apply only to a limited number of masses of specific interest; labeling every detected feature as Level 5 is usually counterproductive.</li> <li>Use blanks to ensure the signal is not from sample preparation or the measurement system.</li> </ul>"},{"location":"workflow/untar_identification/#common-identification-approaches","title":"Common identification approaches","text":"Approach Typical inputs Confidence level(s) Exact mass / formula search precursor $m/z$, adduct assumptions Level 4 (if formula is unequivocal); otherwise Level 5 MS/MS library match experimental MS/MS spectrum Level 2a; can drop to Level 3 if ambiguous Rule-based annotation diagnostic fragments + neutral losses Level 2b when uniquely supported; otherwise Level 3 Standard confirmation (spike-in) standard RT/m/z/MS2/(CCS) Level 1 (confirmed structure)"},{"location":"workflow/untar_identification/#other-approaches","title":"Other approaches","text":"<ul> <li> <p>Network-based approaches: tools like molecular networking can group related features based on MS/MS similarity, which can help propagate annotations and increase confidence in identifications within a network of related compounds. However, the confidence in individual identifications still depends on the quality of the underlying evidence for each feature.</p> </li> <li> <p>Molecular networking can be used to group related features based on MS/MS similarity, which can help propagate annotations and increase confidence in identifications within a network of related compounds. However, the confidence in individual identifications still depends on the quality of the underlying evidence for each feature.</p> </li> <li>Feature-based molecular networking can integrate MS1 features with MS/MS spectra to provide a more comprehensive view of the data, which can aid in the identification process by linking features to their corresponding spectra and facilitating the propagation of annotations within related groups of compounds.</li> <li> <p>MS2LDA can identify recurring fragmentation patterns (Mass2Motifs) that may correspond to specific substructures, which can provide additional evidence for structural hypotheses, especially when combined with other approaches.</p> </li> <li> <p>In silico fragmentation: can generate candidate structures based on accurate mass and predict their MS/MS spectra for comparison with experimental data. Confidence depends on the quality of predictions and the uniqueness of matches.</p> </li> <li> <p>Sirus can generate candidate structures based on accurate mass and predict their MS/MS spectra for comparison with experimental data. Confidence depends on the quality of predictions and the uniqueness of matches.</p> <ul> <li>CSI:FingerID (now part of Sirus) can predict molecular fingerprints from MS/MS spectra and use them to search for candidate structures in databases, which can assist in identification, especially for novel compounds. However, the confidence in these predictions depends on the quality and representativeness of the training data.</li> </ul> </li> <li> <p>metFrag can also generate candidate structures based on accurate mass and predict their MS/MS spectra for comparison with experimental data. Confidence depends on the quality of predictions and the uniqueness of matches.</p> </li> <li> <p>Ion mobility / CCS: can offer orthogonal evidence based on the shape and size of ions, which can help distinguish isomers. However, CCS databases are still limited, and the technique may not be available in all labs.</p> </li> <li> <p>Orthogonal methods: techniques such as NMR, IR, or UV\u2013Vis spectroscopy can provide complementary structural information, especially for confirming key features of a proposed structure. These methods are often more resource-intensive but can significantly increase confidence when used alongside MS-based evidence.</p> </li> <li> <p>Isotopic labeling: can provide evidence for the number of specific atoms (e.g., C, N, S) in a molecule, which can help narrow down candidate structures and increase confidence in identifications.</p> </li> <li> <p>Machine learning models: can be trained to predict structural features or classes based on MS/MS spectra, which can assist in identification, especially for novel compounds. However, the confidence in these predictions depends on the quality and representativeness of the training data.</p> </li> </ul>"},{"location":"workflow/untar_identification_lip/","title":"Identification in lipidomics","text":"<p>Lipid identification follows many of the same principles as metabolomics feature annotation, but it is often more challenging because lipids have extensive structural diversity and many isomeric/isobaric species. This page focuses on rule-based annotation commonly used in lipidomics and how to report results at an appropriate confidence/structural level.</p> <p>Guidance here is aligned with the Lipidomics Standards Initiative (LSI): https://lipidomicstandards.org.</p>"},{"location":"workflow/untar_identification_lip/#key-principles","title":"Key principles","text":"<ul> <li>Use lipid biochemistry as a constraint: consider plausible building blocks for your matrix (fatty acyl chain lengths/unsaturation, sphingoid bases, typical lipid classes) and document any assumptions.</li> <li>Account for isomers (same formula, different structure) and isobars (same nominal/accurate mass from different species).</li> <li>Handle overlaps explicitly:<ul> <li>isotopic overlap (especially 13C within a class)</li> <li>adduct-related overlap (within/between classes)</li> <li>in-source fragments</li> </ul> </li> <li>Use standardized nomenclature:<ul> <li>Lipid shorthand nomenclature: Liebisch et al., J Lipid Res (2013)</li> <li>Fragment ion nomenclature: Pauling et al., PLoS One (2017)</li> </ul> </li> </ul>"},{"location":"workflow/untar_identification_lip/#reporting-level","title":"Reporting level","text":"<p>In untargeted lipidomics, the \u201ccorrect\u201d reporting level depends on data quality (MS1 mass accuracy, MS/MS coverage, separation, ion mode, and standards).</p> <ul> <li>Class / sum composition (e.g., <code>PC 34:1</code>): typically feasible from class-specific fragments/neutral losses + accurate mass constraints.</li> <li>Molecular species / fatty acyl level (e.g., <code>PC 16:0_18:1</code>): requires informative MS/MS (and rules depend on lipid class and adduct/polarity).</li> <li>sn-position / double-bond position: do not report unless supported by an appropriate method and validated evidence.</li> </ul>"},{"location":"workflow/untar_identification_lip/#rule-based-identification","title":"Rule-based identification","text":""},{"location":"workflow/untar_identification_lip/#low-mass-resolution-typically-triple-quadrupole","title":"Low mass resolution (typically triple quadrupole)","text":"<p>Low-resolution identification relies on tandem MS using class-specific product ions or neutral losses (often in MRM workflows). For rule-based annotation:</p> <ul> <li>Show which fragment(s)/neutral loss(es) support the annotation.</li> <li>State the assumptions used to resolve ambiguity (e.g., even-carbon chains, ester-only lipids, typical sphingoid base type).</li> </ul>"},{"location":"workflow/untar_identification_lip/#glycerophospholipids","title":"Glycerophospholipids","text":"<p>Class / sum composition</p> <ul> <li>Requires detection of a class-specific fragment or neutral loss.</li> <li>Any assumptions used to resolve overlap must be stated.</li> </ul> <p>Examples (illustrative):</p> <ul> <li>PIS+ <code>m/z 184</code>:<ul> <li>annotate <code>m/z 746.6</code> as <code>PC O-34:1</code> if you assume only even-carbon chains</li> <li>annotate <code>m/z 746.6</code> as <code>PC 33:1</code> if you assume only ester bonds</li> </ul> </li> <li>PIS+ <code>m/z 196</code>:<ul> <li>annotate <code>m/z 702.5</code> as <code>PE O-34:1</code> (even-carbon assumption)</li> <li>annotate <code>m/z 702.5</code> as <code>PE 33:1</code> (ester-only assumption)</li> </ul> </li> </ul> <p>Molecular species / fatty acyl level</p> <ul> <li>Requires molecular-species informative fragments.</li> <li>Interpretation depends on bond type:<ul> <li>Diacyl: fragments/neutral losses for both acyl chains (as fatty acid or ketene) may be observed.</li> <li>Alkyl-acyl: often only one chain is directly supported.</li> </ul> </li> </ul> <p>Examples (illustrative):</p> <ul> <li>Product ions (\u2212) of <code>m/z 804.6</code> include <code>m/z 255.2</code> and <code>281.2</code> \u2192 <code>PC 16:0_18:1</code> (<code>[M+HCOO]\u2212</code>)</li> <li>Product ions (\u2212) of <code>m/z 716.5</code> include <code>m/z 255.2</code> and <code>281.2</code> \u2192 <code>PE 16:0_18:1</code> (<code>[M\u2212H]\u2212</code>)</li> <li>Product ions (\u2212) of <code>m/z 760.6</code> include <code>m/z 478.3/504.3</code> and/or <code>m/z 496.3/522.3</code> \u2192 <code>PC 16:0_18:1</code> (<code>[M+H]+</code>)</li> </ul> <p>Lysophospholipids</p> <ul> <li>Use class-specific fragments and (where possible) molecular-species fragments.</li> <li>State assumptions used for resolving isobars.</li> </ul> <p>Examples (illustrative):</p> <ul> <li>PIS+ <code>m/z 184</code>: annotate <code>m/z 482.6</code> as <code>LPC O-16:0</code> (even-carbon assumption)</li> <li>PIS+ <code>m/z 184</code>: annotate <code>m/z 482.6</code> as <code>LPC 15:0</code> (ester-only assumption)</li> </ul>"},{"location":"workflow/untar_identification_lip/#sphingolipids","title":"Sphingolipids","text":"<p>Class / sum composition</p> <ul> <li>Requires class-specific fragments.</li> <li>State assumptions (e.g., sphingoid base hydroxylation).</li> </ul> <p>Examples (illustrative):</p> <ul> <li>PIS+ <code>m/z 184</code>: annotate <code>m/z 703.6</code> as <code>SM 34:1</code> assuming a dihydroxy sphingoid base</li> <li>PIS+ <code>m/z 184</code>: annotate <code>m/z 703.6</code> as <code>SM 33:2</code> assuming a trihydroxy sphingoid base</li> </ul> <p>Molecular species</p> <ul> <li>Requires long-chain base and/or acyl-chain informative fragments.</li> </ul> <p>Examples (illustrative):</p> <ul> <li>PIS+ <code>m/z 266</code>: <code>m/z 540.5</code> \u2192 <code>Cer d18:0/16:0</code></li> <li>PIS+ <code>m/z 264</code>: <code>m/z 538.5</code> \u2192 <code>Cer d18:1/16:0</code></li> <li>PIS+ <code>m/z 264</code>: <code>m/z 700.6</code> \u2192 <code>HexCer d18:1/16:0</code></li> <li>PIS+ <code>m/z 264</code>: <code>m/z 862.6</code> \u2192 <code>Hex2Cer d18:1/16:0</code></li> <li>PIS+ <code>m/z 262</code>: <code>m/z 536.5</code> \u2192 <code>Cer d18:2/16:0</code></li> </ul>"},{"location":"workflow/untar_identification_lip/#sterols-eg-cholesteryl-esters","title":"Sterols (e.g., cholesteryl esters)","text":"<ul> <li>Requires sterol-class fragments.</li> <li>Clearly state assumptions (e.g., cholesterol as dominant sterol, ester bond present).</li> </ul> <p>Example (illustrative):</p> <ul> <li>PIS+ <code>m/z 369</code>: annotate <code>m/z 666.6</code> as <code>CE 18:2</code> (<code>[M+NH4]+</code>) assuming cholesterol is the major sterol.</li> </ul>"},{"location":"workflow/untar_identification_lip/#high-mass-resolution-eg-orbitrapqtof","title":"High mass resolution (e.g., Orbitrap/QTOF)","text":"<p>Higher mass resolution improves separation of isobars and increases confidence of molecular formula/adduct assignment, but accurate mass alone is rarely sufficient for structural claims.</p> <p>Identification based on accurate mass is only reasonable when:</p> <ul> <li>it is paired with thorough deconvolution/feature curation, and</li> <li>supported by additional evidence (both ion modes and/or MS/MS).</li> </ul> <p>At sufficient resolution and with appropriate supporting evidence, high-resolution data can help differentiate bond types (e.g., acyl vs alkyl) and certain modifications (e.g., hydroxylation), but fatty acyl-level assignment still generally requires MS/MS.</p> <p>Examples (illustrative):</p> <ul> <li>HR-MS+: annotate <code>m/z 760.5850</code> as <code>PC 34:1</code> when <code>PE 37:1</code> can be excluded by biochemistry and/or a supporting negative-mode adduct (e.g., <code>m/z 804.5760</code> with formate).</li> <li>HR-MS+: annotate <code>m/z 746.6058</code> as <code>PC O-34:1</code> when <code>PE O-37:1</code> can be excluded by biochemistry and/or a supporting negative-mode adduct (e.g., <code>m/z 790.5967</code> with formate).</li> </ul>"},{"location":"workflow/untar_identification_lip/#lipid-class-specific-fragments-rule-of-thumb","title":"Lipid class-specific fragments (rule-of-thumb)","text":"<p>The tables below summarize commonly used diagnostic fragments/neutral losses. Exact observability depends on instrument type, collision energy, adduct, and chromatography.</p>"},{"location":"workflow/untar_identification_lip/#positive-ion-mode-examples","title":"Positive ion mode (examples)","text":"Lipid class Typical precursor/adduct Diagnostic fragment / neutral loss Typical reporting level Notes PC <code>[M+H]+</code> <code>m/z 184.0733</code> (headgroup); NL <code>183.0661</code> Sum composition Isobars may require assumptions (even-carbon, ester-only, etc.) PE <code>[M+H]+</code> NL <code>141.0191</code> Sum composition PS <code>[M+H]+</code> NL <code>185.0089</code> Sum composition PI <code>[M+NH4]+</code> NL <code>277.0563</code> Sum composition PG <code>[M+NH4]+</code> NL <code>189.0402</code> Sum composition PA <code>[M+NH4]+</code> NL <code>115.0035</code> (and sometimes NL <code>17.0266</code>, low specificity) Sum composition DG <code>[M+NH4]+</code> NL <code>35.0371</code> (and sometimes NL <code>17.0266</code>, low specificity) Sum composition SM <code>[M+H]+</code> <code>m/z 184.0733</code> Sum composition Hydroxylation/isomer assumptions may be required Cer/HexCer/Hex2Cer <code>[M+H]+</code> Long-chain-base product ions (e.g., <code>m/z 264.2685</code> for d18:1 backbone) Molecular species (partial) Confirm acyl chain/base with informative fragments"},{"location":"workflow/untar_identification_lip/#negative-ion-mode-examples","title":"Negative ion mode (examples)","text":"Lipid class Typical precursor/adduct Diagnostic fragment / neutral loss Typical reporting level Notes PC <code>[M+HCOO]\u2212</code> NL <code>60.0211</code> Sum composition Also commonly observed as acetate adduct PC <code>[M+CH3COO]\u2212</code> NL <code>74.0368</code> Sum composition PE <code>[M\u2212H]\u2212</code> <code>m/z 140.0118</code> and/or <code>m/z 196.0380</code> Sum composition PG <code>[M\u2212H]\u2212</code> <code>m/z 152.9958</code> and/or <code>m/z 171.0063</code> Sum composition PA <code>[M\u2212H]\u2212</code> <code>m/z 152.9958</code> Sum composition PI <code>[M\u2212H]\u2212</code> <code>m/z 241.0119</code>, <code>223.0013</code>, <code>152.9958</code>, <code>259.0224</code> Sum composition PS <code>[M\u2212H]\u2212</code> <code>m/z 152.9958</code> and/or NL <code>87.0320</code> Sum composition SM <code>[M+HCOO]\u2212</code> or <code>[M+CH3COO]\u2212</code> <code>m/z 168.0431</code> + adduct-specific NL Sum composition"},{"location":"workflow/untar_identification_lip/#novel-lipid-structures","title":"Novel lipid structures","text":"<p>Annotation of novel lipid structures should be based on substantial evidence:</p> <ul> <li>Exclude interference from isomeric and/or isobaric lipid species.</li> <li>Provide fragment-ion evidence supporting both lipid class and structural elements, ideally in both positive and negative ion modes (when ionizable).</li> <li>If proposing a novel lipid class, validate the structure using a synthetic standard when possible.</li> <li>If no standard is available, isolate/purify the compound and confirm by orthogonal methods (e.g., NMR).</li> </ul>"},{"location":"workflow/untar_intro/","title":"Untargeted metabolomics/lipidomics","text":"<p>Untargeted metabolomics/lipidomics aims to detect and semi-quantify a broad set of metabolites/lipids without pre-defining a target list. The output is typically a feature table (samples x features) with feature metadata (e.g., $m/z$, retention time, identity) that can be used for downstream statistics.</p> <p>Because untargeted LC\u2013MS(/MS) data contain substantial technical variation (drift, batch effects, missing values, artifacts), reliable results depend on a transparent processing workflow with clear QC checks.</p>"},{"location":"workflow/untar_intro/#overview","title":"Overview","text":"<p>The workflow below describes the recommended sequence used in this documentation set. Each step links to a dedicated page with implementation details and suggested checks.</p> <ul> <li> <p>Preprocessing: raw-data conversion and feature detection (peak picking), alignment, deconvolution, and initial filtering.</p> </li> <li> <p>Identification</p> <ul> <li>General: feature annotation/identification concepts, reporting levels, and practical checks.</li> <li>Lipid annotation: rule-based lipidomics annotation guidance and reporting level recommendations.</li> </ul> </li> <li> <p>Quality control: review pooled QCs, blanks, drift/outliers, and overall acquisition stability before committing to downstream statistics.</p> </li> <li> <p>Post-processing</p> <ul> <li>Peak cleaning: remove artifacts and low-quality features (e.g., blank contaminants, unstable features, unreliable RT regions).</li> <li>Missing imputation: evaluate missingness mechanisms (MNAR/MAR/MCAR), filter where appropriate, then impute with documented assumptions.</li> <li>Normalization: reduce technical variation (drift/batch) while preserving biological signal; evaluate improvement using QC metrics.</li> <li>Merge peaklist: merge positive/negative mode results (and/or batches) and attach consistent compound metadata for the final analysis-ready table.</li> </ul> </li> </ul>"},{"location":"workflow/untar_merge/","title":"Merge peaklist and map information","text":"<p>After post-processing (cleaning, missingness filtering/imputation, normalization), the processed peaklists from positive and negative ion mode are merged into a single final table. The merge step should (1) avoid duplicating the same compound reported in both modes, and (2) attach consistent compound metadata from the identification/library results.</p> <p>The exact merge logic depends on how identification is represented in your pipeline (feature-level annotation vs compound-level identification). The procedure below assumes you have a column that represents the same compound identity in both modes (e.g., InChIKey, library ID, or curated compound name).</p>"},{"location":"workflow/untar_merge/#general-procedures","title":"General procedures","text":""},{"location":"workflow/untar_merge/#1-harmonize-identifiers-and-columns","title":"1) Harmonize identifiers and columns","text":"<p>Before merging, ensure both mode-specific peaklists use the same:</p> <ul> <li>sample column names and sample order</li> <li>intensity scale (raw vs log) and normalization status</li> <li>key identifier columns (e.g., InChIKey, library ID) that will be used for merging</li> </ul>"},{"location":"workflow/untar_merge/#2-remove-redundant-feature-entries-across-compounds","title":"2) Remove redundant feature entries across compounds","text":"<p>If your peaklists are still at the feature level (m/z\u2013RT) rather than compound level, you may have multiple features corresponding to the same compound (e.g., different adducts, isotopes, or in-source fragments). Consider removing redundant features to simplify downstream analysis.</p> <ul> <li>If you have identified adducts/in-source fragments, keep only one representative feature per compound (e.g., the most abundant adduct).</li> </ul>"},{"location":"workflow/untar_merge/#3-remove-redundant-compound-entries-across-modes","title":"3) Remove redundant compound entries across modes","text":"<p>If a compound is detected/annotated in both modes, choose a single representative entry per compound.</p> <p>Suggested decision order (use the first applicable rule and document it):</p> <ul> <li>Prefer higher-confidence identification (e.g., MS/MS match score, identification level, or manual validation flag).</li> <li>If confidence is similar, prefer the mode with better analytical quality:<ul> <li>lower pooled-QC CV% (more precise)</li> <li>lower missingness in study samples</li> </ul> </li> <li>If quality is similar, keep the entry with higher typical signal in study samples (e.g., higher median/mean intensity).</li> </ul> <p>Notes:</p> <ul> <li>Only merge across modes when you are confident two rows represent the same compound identity. If identity is uncertain, keep both and mark the mode/adduct explicitly.</li> <li>If you still work at feature level (m/z\u2013RT) rather than compound level, avoid merging solely by similar m/z/RT across modes; positive/negative adduct chemistry makes that unreliable.</li> </ul>"},{"location":"workflow/untar_merge/#4-map-compound-metadata-from-the-libraryannotation-table","title":"4) Map compound metadata from the library/annotation table","text":"<p>Attach compound-related metadata to the final output table (ideally Metabolomics Workbench IDs). Typical fields include:</p> <ul> <li>Molecular formula</li> <li>Monoisotopic mass</li> <li>HMDB ID</li> <li>KEGG ID</li> <li>PubChem CID</li> <li>SMILES</li> <li>InChIKey</li> <li>Metabolite / lipid class</li> </ul> <p>If multiple library hits exist for the same key (e.g., synonyms or isomers), define a rule to select a primary record (e.g., highest score or curated preferred name) and keep alternative hits in separate columns if needed.</p>"},{"location":"workflow/untar_missing_imputation/","title":"Missing imputation","text":"<p>Missing values can arise for multiple reasons in metabolomics/lipidomics, such as true absence/very low abundance (below detection), ion suppression, peak picking/alignment failures, or sporadic acquisition issues. Evaluating the extent and pattern of missingness is essential before choosing an imputation strategy, because inappropriate imputation can bias fold-changes, variance, and downstream statistics.</p>"},{"location":"workflow/untar_missing_imputation/#missing-evaluation","title":"Missing evaluation","text":"<p>Before imputation, perform a thorough missingness evaluation to understand the underlying mechanisms and to inform method choice. This includes:</p>"},{"location":"workflow/untar_missing_imputation/#missing-value-summary","title":"Missing value summary","text":"<p>Generate summaries at both feature and sample level:</p> <ul> <li>Per feature: fraction missing overall and within each biological group.</li> <li>Per sample: fraction missing across all features (helps detect failed injections or problematic samples).</li> <li>By sample type: study samples vs pooled QC vs blanks.</li> </ul>"},{"location":"workflow/untar_missing_imputation/#missing-value-distribution","title":"Missing value distribution","text":"<p>Visualize missingness to identify structure:</p> <ul> <li>heatmap of missingness (features \u00d7 samples)</li> <li>histogram of % missing per feature</li> <li>missingness vs run order (to reveal drift-related dropouts)</li> </ul>"},{"location":"workflow/untar_missing_imputation/#correlation-with-sample-metadata","title":"Correlation with sample metadata","text":"<p>Explore associations between missingness and metadata:</p> <ul> <li>batch, run order, sample type, instrument day</li> <li>biological group (note: true biology can change detectability)</li> </ul> <p>If missingness is strongly batch- or run-order dependent, address QC/batch issues (peak picking, alignment, drift correction) before relying on statistical imputation.</p>"},{"location":"workflow/untar_missing_imputation/#recommended-pre-filters-before-imputation","title":"Recommended pre-filters (before imputation)","text":"<p>Common filters (choose thresholds appropriate for your study and document them):</p> <ul> <li>require a feature to be present in a minimum fraction of samples within at least one group</li> <li>remove samples with unusually high missingness (potential injection/prep failures)</li> </ul>"},{"location":"workflow/untar_missing_imputation/#types-of-missing-values","title":"Types of missing values","text":"<p>In practice, missingness mechanisms can be mixed. Use these categories as guidance for method selection and for how you interpret results.</p> <ul> <li> <p>MNAR (Missing Not At Random)</p> <ul> <li>Typical cause: signal is below the LOD/LOQ (or below peak-picking threshold).</li> <li>Pattern: missingness increases for low-abundance features and/or in specific groups where the feature is low.</li> </ul> </li> <li> <p>MAR (Missing At Random)</p> <ul> <li>Typical cause: feature presence depends on other observed variables (batch, matrix, co-elution, signal suppression) or pipeline decisions.</li> <li>Pattern: missingness correlates with metadata or intensity-related factors.</li> </ul> </li> <li> <p>MCAR (Missing Completely At Random)</p> <ul> <li>Typical cause: sporadic stochastic failures (injection issue, transient ion source instability).</li> <li>Pattern: missing values appear scattered with no clear dependency.</li> </ul> </li> </ul>"},{"location":"workflow/untar_missing_imputation/#missing-imputation-method","title":"Missing imputation method","text":"<p>Imputation is used to enable statistical analysis and visualization, but it also introduces assumptions. Whenever possible, validate that conclusions are robust to reasonable imputation choices.</p>"},{"location":"workflow/untar_missing_imputation/#for-mnar","title":"For MNAR","text":"<p>Use methods that respect the idea that missing values are low, not \u201caverage\u201d. Common options:</p> <ul> <li>LOD/LOQ-based substitution: replace missing with a small value such as LOD, LOQ, or a fraction thereof.</li> <li>HM (half-minimum): replace missing with half of the minimum observed value (globally or within feature).</li> <li>Min/Small constant: replace with a small constant near the noise floor.</li> </ul> <p>Notes:</p> <ul> <li>Prefer using a feature-wise small value tied to the observed distribution/noise floor rather than a single global constant.</li> <li>After substitution, use log transform cautiously: small-value choices can strongly affect fold changes for sparse features.</li> </ul>"},{"location":"workflow/untar_missing_imputation/#for-marmcar","title":"For MAR/MCAR","text":"<p>These methods use relationships across samples/features.</p> <ul> <li>KNN: imputes using nearest neighbors in sample space.</li> <li>RF (Random forest): imputes using predictive models trained on observed values.</li> <li>Other multivariate/low-rank approaches (e.g., PCA-based imputation) when missingness is moderate.</li> </ul> <p>Notes:</p> <ul> <li>Avoid complex imputation when missingness is extreme; consider filtering instead.</li> <li>Evaluate impact on variance: some methods can shrink variance or create overly smooth patterns.</li> </ul>"},{"location":"workflow/untar_normalization/","title":"Normalization","text":"<p>Normalization aims to remove unwanted technical variation (e.g., signal drift, injection-to-injection variability, and batch effects) while preserving true biological differences.</p> <p>In typical workflows, normalization is applied before downstream steps such as data transformation and scaling.</p> <p>Signal drift and batch effect</p> <p></p>"},{"location":"workflow/untar_normalization/#methods","title":"Methods","text":""},{"location":"workflow/untar_normalization/#general-global-scaling","title":"General (global scaling)","text":"<p>Global scaling methods (sum/mean/median) apply a single sample-wise scaling factor so that all samples have the same total intensity (or the same mean/median intensity).</p> <ul> <li>Typical use: when most features are expected not to change strongly across samples.</li> <li>Risk: if a large fraction of features truly changes (e.g., strong global shifts), global scaling can remove biological signal.</li> </ul> <p>Common choices</p> <ul> <li>sum: forces the sum of intensities per sample to be equal.</li> <li>mean: forces the mean intensity per sample to be equal.</li> <li>median: forces the median intensity per sample to be equal (often more robust than mean).</li> <li>mTIC: modified total ion current scaling; a TIC-style sample-wise scaling intended to be more robust than raw TIC when a subset of features is unstable.</li> </ul> <p>Calculation (conceptual)</p> <p>For feature intensity $I_{m,n}$ (feature $m$, sample $n$):</p> <ul> <li>Compute a sample-wise scaling factor $s_n$ (e.g., sample sum / global reference sum).</li> <li>Normalize: $I'{m,n} = I / s_n$.</li> </ul>"},{"location":"workflow/untar_normalization/#internal-standard-based","title":"Internal-standard based","text":"<p>Internal-standard (IS) normalization scales each feature using one or more spiked compounds expected to track technical variation.</p> <ul> <li>Typical use: targeted/quantitative workflows or untargeted workflows with good IS coverage.</li> <li>Requirement: IS should be stable and behave similarly to analytes (ideally class- and RT-matched).</li> </ul> <p>Practical strategy</p> <ul> <li>Select the \u201cbest\u201d IS (or IS set) based on low CV in pooled QC samples.</li> <li>Methods such as NOMIS use multiple IS to improve robustness.</li> </ul> <p>Common choices</p> <ul> <li>bestis: normalization each feature using the \"best\" internal standard (the one that shows the highest correlation with the feature across QC samples).</li> <li>lowCV: normalization using the internal standard with the lowest CV in pooled QCs.</li> <li>nomis: normalization using multiple internal standards (multi-IS strategy).</li> </ul> <p>Calculation (conceptual)</p> <p>For a chosen internal standard signal $IS_{p,n}$ in sample $n$:</p> <ul> <li>$I'{m,n} = I$} / IS_{p,n</li> </ul>"},{"location":"workflow/untar_normalization/#sample-wise-dilution-size-effect-normalization","title":"Sample-wise dilution / size-effect normalization","text":"<p>These methods aim to correct sample-to-sample dilution or total-amount differences that affect many features.</p> <ul> <li>PQN (probabilistic quotient normalization): estimates a sample-specific dilution factor from the distribution of feature-wise quotients relative to a reference (often a pooled QC or a median sample) and scales the sample accordingly.</li> </ul> <p>Note: PQN assumes that most metabolites do not change strongly and that the dominant difference is a global dilution/size effect.</p>"},{"location":"workflow/untar_normalization/#distribution-based-between-sample-normalization","title":"Distribution-based (between-sample) normalization","text":"<p>These methods reshape sample distributions to make them more comparable. Use with care if global shifts are expected.</p> <ul> <li>quantile: forces samples to have the same empirical distribution (common in transcriptomics; can be useful for strongly distribution-driven batch effects but may remove real global biology).</li> <li>contrast: a contrast-based rescaling approach used in some -omics toolchains to reduce distributional differences (implementation varies; document the software and parameters).</li> <li>liwong: Li\u2013Wong style normalization (originating from microarray processing); if used, document the exact implementation because details differ across tools.</li> </ul>"},{"location":"workflow/untar_normalization/#linear-batch-effect-adjustment","title":"Linear batch-effect adjustment","text":"<p>Linear methods explicitly model batch effects and remove them.</p> <ul> <li>Examples: limma, ComBat.</li> <li>Idea: treat batch as a covariate in a linear model and adjust the data accordingly.</li> <li>Requirement: batch structure must be known and reasonably represented in the data.</li> </ul> <p>Additional model-based methods sometimes offered by tools:</p> <ul> <li>linear: a generic label for linear-model adjustment.</li> </ul>"},{"location":"workflow/untar_normalization/#non-linear-drift-correction","title":"Non-linear drift correction","text":"<p>Non-linear methods model intensity-dependent drift, often using QC injections across the run.</p> <ul> <li>Example: LOESS/LOWESS.</li> <li>Idea: model paired differences (often on an MA plot: log fold-change vs mean intensity) as a smooth function of intensity/time.</li> </ul> <p>Additional non-linear options you may encounter:</p> <ul> <li>cubic: cubic smoothing/interpolation-based correction.</li> <li>batch_loess: LOESS-based correction applied within batches (typically leveraging QC injections and run order).</li> <li>batch_ratio: ratio-based within-batch correction relative to QC/reference samples.</li> </ul>"},{"location":"workflow/untar_normalization/#signal-dependent-non-linear","title":"Signal-dependent non-linear","text":"<p>Signal-dependent non-linear methods correct drift using distribution/quantile behavior.</p> <ul> <li>Example: Q-spline.</li> <li>Idea: model paired data at multiple quantiles as a function of mean intensity to handle intensity-dependent effects.</li> </ul>"},{"location":"workflow/untar_normalization/#machine-learning-based-drift-correction","title":"Machine-learning based drift correction","text":"<p>These approaches learn drift patterns from QC injections and correct sample intensities accordingly.</p> <ul> <li>SERRF: QC-based machine-learning drift correction (random-forest style approaches are commonly used) designed to reduce run-order/batch drift.</li> <li>svm: support vector machine-based correction (implementation varies; typically uses QC/run-order information to model drift).</li> </ul> <p>Note: ML-based correction can be powerful but should be validated carefully (e.g., improvement in pooled-QC RSD without collapsing biological group separation).</p>"},{"location":"workflow/untar_normalization/#evaluation","title":"Evaluation","text":"<p>Use both visual and quantitative checks, focusing on QC samples.</p> <p>Recommended evaluation is before vs after normalization, using the same feature table and QC definitions.</p>"},{"location":"workflow/untar_normalization/#visual-checks","title":"Visual checks","text":"<ul> <li>PCA (colored by sample type and batch): pooled QC injections should cluster tightly after normalization; separation by batch should be reduced.</li> <li>PCA (colored by run order/time): check whether run-order trends are reduced.</li> <li>Run-order drift plots (QC only): for a set of representative features (or summary statistics), plot intensity vs injection order.</li> <li>Boxplots/density plots:<ul> <li>Across samples: distributions should become more comparable.</li> <li>Watch-out: forcing distributions to match too strongly (e.g., aggressive distribution normalization) can remove real global biology.</li> </ul> </li> </ul>"},{"location":"workflow/untar_normalization/#quantitative-qc-metrics","title":"Quantitative QC metrics","text":"<p>Compute metrics primarily on pooled QC injections (or on technical replicates).</p> <ul> <li>RSD% in pooled QC per feature:<ul> <li>the distribution of QC RSD shifts lower after normalization.</li> </ul> </li> <li>MAD / robust variance in pooled QC:<ul> <li>Prefer robust metrics when outliers exist.</li> </ul> </li> <li>QC correlation:<ul> <li>Pairwise correlation between pooled QC injections should increase after normalization.</li> </ul> </li> </ul>"},{"location":"workflow/untar_normalization/#overcorrection-checks","title":"Overcorrection checks","text":"<ul> <li>Known biology / positive controls: if you expect specific changes, ensure they are not eliminated.</li> <li>Internal standards: their behavior should become more consistent, but IS correction should not create implausible shifts in unrelated features.</li> <li>Biological group structure:<ul> <li>Normalization should reduce technical separation (batch/run-order) without collapsing real biological differences.</li> <li>If groups become indistinguishable only after normalization, re-check assumptions and method choice.</li> </ul> </li> </ul>"},{"location":"workflow/untar_preprocessing/","title":"Preprocessing","text":"<p>Untargeted preprocessing transforms raw LC\u2013MS(/MS) data into a feature table. In practice, it consists of detecting chromatographic peaks, aligning them across samples, filling missing values created by detection limits, and annotating features using MS1/MS2 information.</p> <p>In general, the preprocessing procedure is shown below (figure from mzmine documentation). Different software tools implement similar steps with varying algorithms and parameters, but the overall workflow is relatively consistent across platforms.</p> <p></p>"},{"location":"workflow/untar_preprocessing/#procedures","title":"Procedures","text":"<p>Below is a practical description of the workflow shown above. Exact module names and parameters vary by software you use. If your dataset does not include ion mobility (IMS), steps related to mobility can be skipped.</p> <p>1. Data import</p> <p>Load vendor/open-format raw files and verify MS1/MS2 levels, polarity, and metadata (sample names, injection order, batch).</p> <p>2. Mass detection</p> <p>Convert raw spectra into centroid \u201cmass lists\u201d by applying a noise threshold. This strongly affects sensitivity and false positives.</p> <p>3. Mobility scan merging (IMS only)</p> <p>Combine mobility-resolved scans as needed to improve signal quality and define the unit of downstream feature detection. The common approach is to merge scans within a narrow RT window (e.g., 0.1\u20130.2 min) to create a single \u201cchromatogram\u201d for each m/z value, which is then used for peak detection and alignment.</p> <p>4. EIC building</p> <p>Build extracted ion chromatograms (EICs) by grouping signals across retention time within an m/z tolerance.    - Practical considerations:      - Choose an m/z tolerance appropriate for your instrument mode (often 10 ppm for high-resolution data).      - Ensure consistent settings across samples (especially when multiple batches/instruments are involved).</p> <p></p> <p>5. Smoothing</p> <p>Reduce high-frequency noise in chromatograms to stabilize peak shape and improve peak boundary detection.</p> <p>6. Resolving (chromatographic peak deconvolution)</p> <ul> <li>Detect chromatographic peaks (features) in EICs and deconvolute overlapping peaks based on shape and spectral consistency.</li> <li>Key parameters (tool-specific names vary):<ul> <li>Noise threshold / minimum intensity</li> <li>Minimum peak width and expected chromatographic peak shape</li> <li>Mass/RT tolerances used during deconvolution</li> </ul> </li> </ul> <p></p> <p>Common issues</p> <ul> <li> <p>Co-eluting metabolites</p> <p></p> <p>Example of deconvolution: three overlapping peaks are separated using mass spectral information, yielding a peak table with responses for each individual metabolite and their corresponding spectra.</p> </li> <li> <p>False positive peaks</p> <p></p> <p>Common causes include random noise, unstable baselines, and integration of non-peak structures. Tighten noise thresholds and/or peak shape constraints if false positives dominate.</p> </li> <li> <p>Difficulty estimating boundaries</p> <p></p> <p>Incorrect boundaries bias peak areas and increase variability. Check a subset of representative peaks (high, medium, low abundance; different RT regions) to confirm settings.</p> </li> </ul> <p>7. IMS expanding (IMS only)</p> <p>Expand chromatographic features into the mobility dimension (or reconstruct mobility-resolved features).</p> <p>8. Smoothing (IMS only)</p> <p>Apply smoothing in the mobility dimension to reduce noise and stabilize mobility peak shapes.</p> <p>9. Resolving (IMS only)</p> <p>Detect/resolve mobility peaks to separate co-migrating signals in the IMS dimension.</p> <p>10. 13C isotope filtering</p> <p>Remove or flag likely isotope signals (e.g., M+1/M+2 patterns) to reduce redundant features.</p> <p>11. Isotope finder</p> <p>Group isotopologues belonging to the same feature and annotate isotope relationships.</p> <p>12. Join aligner</p> <p>Align features across samples using retention time and m/z tolerances so the same compound is represented consistently.</p> <ul> <li> <p>Practical considerations:</p> <ul> <li>Inspect before/after alignment plots to confirm drift correction is plausible.</li> <li>Use pooled QCs to validate that alignment reduces RT scatter without over-warping.</li> </ul> </li> </ul> <p> </p> <p>13. Gap filling</p> <p>Recover intensities for aligned features that were missed during detection by integrating signal in the expected RT/m/z window.</p> <ul> <li> <p>Practical considerations:</p> <ul> <li>Prefer gap filling methods that integrate only within a constrained RT/m/z window.</li> <li>Track how gap filling changes missingness and distributions; do not use it to mask systematic problems (e.g., misalignment).</li> </ul> </li> </ul> <p></p> <p>14. Duplicate filter</p> <p>Remove redundant features produced by overlapping detection/grouping (e.g., duplicate entries with nearly identical RT and m/z).</p> <p>15. Feature grouping</p> <p>Group features from the same compound (e.g., adducts, isotopes, in-source fragments) based on co-elution. This reduces redundancy and supports annotation.</p> <p>16. Ion identity networking (adduct/complex linking)</p> <p>Link features that likely represent the same molecule as different adducts/in-source fragments; supports cleaner annotation and reduces redundancy.</p> <p>17/18. Spectral library search (MS/MS)</p> <p>Assign chemical meaning to features using MS/MS information. This step will be explained in more detail in the identification section. Here we focus on the general concept.</p> <p></p> <ul> <li>MS2 annotations use fragment spectra to increase confidence via library matching.</li> </ul> <p></p> <ul> <li>If your acquisition uses multiple collision energies, compare matches across energies (fragment patterns can shift with energy).</li> </ul> <p></p> <p>19. Local database search (MS1)</p> <p>If you have a custom spectral library (e.g., from standards or previous experiments), search against it to assign known identities to features based on accurate mass and retention time (and ccs).</p> <p>20. Lipid annotation (lipidomics only)</p> <p>Use rule based annotation to assign lipid classes and sum compositions/lipid species based on MS1/MS2 patterns. This step will be covered in more detail in the identification section, but the general idea is to use characteristic fragments and neutral losses to classify lipids and assign acyl chain compositions where possible.</p> <p>21. Feature filtering</p> <p>This will be covered in more detail in the post processing section.</p>"},{"location":"workflow/untar_qc/","title":"Quality control (QC)","text":"<p>QC in metabolomics/lipidomics aims to detect and control technical variation (sample preparation, instrument drift, batch effects, carryover, contamination) so that biological interpretation is reliable.</p> <p>This page summarizes common QC approaches. Use thresholds appropriate for your platform and study design, and always document the rules you apply.</p>"},{"location":"workflow/untar_qc/#common-qc-sample-types","title":"Common QC sample types","text":"<p>This has been covered in the QC samples setup section.</p>"},{"location":"workflow/untar_qc/#qc-procedures","title":"QC procedures","text":""},{"location":"workflow/untar_qc/#inspection-of-blank-samples","title":"Inspection of blank samples","text":"<p>Use procedural blanks to identify molecular features driven by background contamination and carryover, rather than the biological matrix.</p> <ul> <li>Common sources of blank signal:</li> <li>Solvent/reagent impurities</li> <li>Sample-prep artifacts (e.g., derivatization \u201cghost peaks\u201d)</li> <li>Labware and LC\u2013MS system contamination (e.g., plasticizers, column bleed)</li> <li> <p>Carryover due to insufficient washing (features appearing in blanks after high-abundance injections)</p> </li> <li> <p>Flag potential contaminants (screening rule):</p> </li> <li>Flag features as \u201cpotential contaminants\u201d when their mean intensity in real samples does not exceed 2\u20133\u00d7 the mean intensity in blanks (i.e., low sample/blank ratio).</li> <li>If filtering is needed, consider discarding only flagged features with high variability in blanks (e.g., blank RSD &gt; 15%). Stable blank signals are less likely to differentially bias group comparisons, but should still be interpreted with caution for features of interest.</li> </ul>"},{"location":"workflow/untar_qc/#inspection-of-internal-standards","title":"Inspection of internal standards","text":"<p>Use internal standards to detect run-order drift and out-of-control events that indicate unstable instrument performance (e.g., sensitivity loss, chromatographic deterioration, sudden failures such as clogs).</p> <ul> <li>Run-order drift (gradual changes):</li> <li> <p>Plot IS intensity, RT, and m/z vs injection order to identify time-related trends.</p> </li> <li> <p>Out-of-control measurements (sudden changes):</p> </li> <li>Create Shewhart control charts for key IS metrics with warning/action limits (\u00b11 SD, \u00b12 SD, \u00b13 SD).</li> <li>Flag problematic segments, for example:<ul> <li>1 QC/IS point outside \u00b13 SD</li> <li>2 consecutive points outside \u00b12 SD on the same side of the mean</li> <li>4 consecutive points outside \u00b11 SD on the same side of the mean</li> <li>10 consecutive points on the same side of the mean</li> </ul> </li> <li>If QC/IS metrics fail these rules, scrutinize neighboring study injections and decide whether they should be excluded and reanalyzed, since abrupt failures are often not fully correctable by normalization.</li> </ul> <p></p>"},{"location":"workflow/untar_qc/#inspection-of-pooled-qcs","title":"Inspection of pooled QCs","text":"<ul> <li>Plot signal intensities with respect to the run order to check for time-related drifts.</li> <li>Examine PCA scores plot to check the distribution of samples.</li> <li>Use PCA scores (colored by run order and sample type) to assess whether samples show a continuous drift across the run. Pooled QCs should ideally cluster tightly (often near the center), indicating stable performance.</li> </ul>"},{"location":"workflow/untar_qc/#pooled-qc-dilution-series-checks-linearity-and-dynamic-range","title":"Pooled QC dilution series checks (linearity and dynamic range)","text":"<p>If you run pooled-QC dilutions:</p> <ul> <li>Check that features show monotonic response with dilution.</li> <li>Use this to identify features dominated by noise/saturation and to support more defensible filtering.</li> </ul>"},{"location":"workflow/untar_qc/#inspection-of-outliers","title":"Inspection of outliers","text":"<p>Outliers can arise from sample preparation issues, instrument problems, or true biological extremes. Here we focus on technical outliers that may need to be excluded or reanalyzed. - Samples located outside the 95% Hotelling T\u00b2 ellipse in the PCA scores plot. - IS intensity outside a robust range (e.g., median \u00b1 3\u00d7MAD) - Unusually high/low total IS signal (sum/median across IS) - RT shift beyond tolerance for multiple IS</p>"},{"location":"workflow/untar_qc/#pre-analytical-markers","title":"Pre-analytical markers","text":"<ul> <li>Create box plots and check for abnormal levels (\u00b13\u00d7IQR).</li> </ul> <p>Pre-analytical handling (collection, processing, aliquoting, transport, storage, and freeze\u2013thaw cycles) is a major source of variability\u2014especially in multicenter and biobank studies\u2014and can cause contamination, degradation, and ex vivo metabolic changes that are not addressed by analytical QC alone. Common issues include insufficient quenching (continued enzymatic activity such as lipid hydrolysis), oxidation/light- or air-induced transformations of labile metabolites, delayed blood processing or poor temperature control (shifts in energy metabolites), hemolysis (release of intracellular metabolites), and urine degradation due to bacterial overgrowth.</p> <p>To monitor sample integrity, track a panel of pre-analytical marker metabolites known to respond to these errors and visualize them using box plots to flag abnormal values (e.g., intensities outside \u00b13\u00d7 IQR). Because these markers can also vary biologically, interpret them as a pattern (multiple markers) before excluding samples, and adapt the marker panel to the biological matrix (blood/urine or tissue-specific markers).</p> <p></p>"},{"location":"workflow/untar_qc/#overall-data-quality-assessment","title":"Overall data quality assessment","text":"<ul> <li>Verify the clustering of QCs in PCA scores plots.</li> <li>Check method precision and accuracy:</li> <li>RSD &lt; 30% for peak intensity</li> <li>RSD &lt; 2% for retention time</li> <li>RSD &lt; 15% for peak width</li> <li>m/z error &lt; 10 ppm</li> <li>Create time series plots with predefined tolerance windows to visualize reproducibility and stability across the experiment for every quality metric.</li> <li>Flag features with technical variation exceeding the biological inter-individual variability.</li> </ul>"},{"location":"workflow/untar_rt_prediction/","title":"Retention time prediction","text":"<p>The retention time acquired in the chromatographic system might shift due to the change of mobile phase, instrument conditions, columns, etc. Therefore, we need to update the library, i.e. predict the new retention time. This can be achieved by using a set of standard compounds with known retention times to build a prediction model, which is then used to predict the retention times of other compounds in the library.</p> <p></p>"}]}